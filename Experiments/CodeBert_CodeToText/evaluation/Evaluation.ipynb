{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CodeBert Grid Experiment Evaluation\n",
    "\n",
    "**A full run of this Notebook takes about 40 minutes on my machine.**\n",
    "\n",
    "Make sure to have all required dependencies installed - they are listed in the [environment.yml](./environment.yml). \n",
    "You create a conda environment from the yml using \n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate Lampion-Codebert-Evaluation\n",
    "```\n",
    "\n",
    "Make sure to run your Jupyter Notebook from that environment! \n",
    "Otherwise you are (still) missing the dependencies. \n",
    "\n",
    "**OPTIONALLY** you can use the environment in which your jupter notebook is already running, with starting a new terminal (from jupyter) and run \n",
    "\n",
    "```\n",
    "conda env update --prefix ./env --file environment.yml  --prune\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Steps\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "The following Steps need to be adjusted for the run to finish: \n",
    "    \n",
    "    1. Run metric Runner external in case you are on windows\n",
    "    2. Change data directories to required\n",
    "    3. Change Config Archetypes to match what you have, will be used for printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that by the end of this notebook we create **a big .csv file.**\n",
    "\n",
    "Some of the statistical tests where easier to do in R, which is provided in a seperate file starting from the bleus.csv created by the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "# Homebrew Imports (python-file next to this)\n",
    "import bleu_evaluator as foreign_bleu\n",
    "\n",
    "# Set Jupyter vars\n",
    "# %matplotlib notebook\n",
    "plt.rcParams.update({'font.size': 35})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data-Loading / Preparation\n",
    "\n",
    "Make sure that your dataset looks like described in the [Readme](./README.md), that is \n",
    "\n",
    "```\n",
    "./data\n",
    "    /PaperResults\n",
    "        /configs\n",
    "            /reference\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_0\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_1\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "    ...\n",
    "```\n",
    "\n",
    "where the configs **must** be numbered to be correctly detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This runs the bleu-score upon the config files, creating the bleu.txt's \n",
    "# If your data package was provided including the txt you dont need to do this. \n",
    "# Existing bleu.txt's will be overwritten. \n",
    "\n",
    "# Note: This did not behave as intended on Windows - run the command per extraneous bash\n",
    "\n",
    "#!./metric_runner.sh ./data/java_results/\n",
    "#!./metric_runner.sh ./data/python_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells first run over the given data directory and read all paths, \n",
    "then the properties and finally all of the data is loaded.\n",
    "\n",
    "The bleu.txt files are required at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The directory where to look for the data, default are the paper results\n",
    "# Expected format: \n",
    "# [(\"Prefix\",\"Path\"),(\"Prefix2\",\"Path2\")]\n",
    "data_directories = [(\"java\",\"./data/java_results\"),(\"python\",\"./data/python_results\")]\n",
    "\n",
    "# These archetypes are later used to group the configurations\n",
    "# While to grouping is up to you, right here it simply is one archetype for each transformation type, \n",
    "# Grouping together different configs with the same transformations applied (but different #Transformations)\n",
    "\n",
    "# These were the archetypes for the first run (presented in ASE2022 NIER Track)\n",
    "#config_archetypes = {\n",
    "#    \"config_0\":\"if\",\"config_1\":\"if\",\"config_2\":\"if\",\n",
    "#    \"config_3\":\"neutral-element\",\"config_4\":\"neutral-element\",\"config_5\":\"neutral-element\",\n",
    "#    \"config_6\":\"mixed names(pseudo)\",\"config_7\":\"mixed names(pseudo)\",\"config_8\":\"mixed names(pseudo)\",\n",
    "#    \"config_9\":\"mixed-names(random)\",\"config_10\":\"mixed-names(random)\",\"config_11\":\"mixed-names(random)\",\n",
    "#    \"config_12\": \"add-var(pseudo)\",\"config_13\": \"add-var(pseudo)\",\"config_14\": \"add-var(pseudo)\",\n",
    "#    \"config_15\": \"add-var(random)\",\"config_16\": \"add-var(random)\",\"config_17\": \"add-var(random)\",\n",
    "#    \"config_18\": \"if & neutral-element\",\"config_19\": \"if & neutral-element\",\"config_20\": \"if & neutral-element\"\n",
    "#}\n",
    "# These are the Archetypes for the second run (To be submitted 2022)\n",
    "config_archetypes = {\n",
    "    \"config_0\":\"if\",\"config_1\":\"if\",\"config_2\":\"if\",\n",
    "    \"config_3\":\"lambda\",\"config_4\":\"lambda\",\"config_5\":\"lambda\",\n",
    "    \"config_6\":\"neutral-element\",\"config_7\":\"neutral-element\",\"config_8\":\"neutral-element\",\n",
    "    \"config_9\":\"mixed names(pseudo)\",\"config_10\":\"mixed names(pseudo)\",\"config_11\":\"mixed names(pseudo)\",\n",
    "    \"config_12\":\"mixed-names(random)\",\"config_13\":\"mixed-names(random)\",\"config_14\":\"mixed-names(random)\",\n",
    "}\n",
    "\n",
    "print(f\"looking for results in {data_directories}\" )\n",
    "\n",
    "results={}\n",
    "\n",
    "for (prefix,data_directory) in data_directories:\n",
    "    print(f\"Looking for {prefix} at {data_directory}\")\n",
    "    results[prefix]={}\n",
    "    for root,dirs,files in os.walk(data_directory):\n",
    "        for name in files:\n",
    "            if \".gold\" in name:\n",
    "                directory = os.path.basename(root)\n",
    "                results[prefix][directory]={}\n",
    "                results[prefix][directory][\"prefix\"]=prefix\n",
    "                results[prefix][directory][\"result_file\"]=os.path.join(root,\"test_0.output\")\n",
    "                results[prefix][directory][\"gold_file\"]=os.path.join(root,\"test_0.gold\")\n",
    "                results[prefix][directory][\"bleu_file\"]=os.path.join(root,\"bleu.txt\")\n",
    "                if os.path.exists(os.path.join(root,\"config.properties\")):\n",
    "                    results[prefix][directory][\"property_file\"]=os.path.join(root,\"config.properties\")\n",
    "                \n",
    "print(f\"Found {len(results.keys())} configuration folders in {data_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    \"\"\"\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    \"\"\"\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "print(\"reading in property-files\")\n",
    "\n",
    "for prefix in results.keys():\n",
    "    for key in results[prefix].keys():\n",
    "        if \"property_file\" in results[prefix][key].keys():\n",
    "            results[f\"{prefix}\"][f\"{key}\"][\"properties\"]=load_properties(results[prefix][key][\"property_file\"])\n",
    "\n",
    "print(\"done reading the properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"reading in result-files\")\n",
    "\n",
    "for prefix in results.keys():\n",
    "    for key in results[prefix].keys():\n",
    "        result_file = results[prefix][key][\"result_file\"]\n",
    "        f = open(result_file)\n",
    "        lines=f.readlines()\n",
    "        results[prefix][key][\"results\"]={}\n",
    "        for l in lines:\n",
    "            num = int(l.split(\"\\t\")[0])\n",
    "            content = l.split(\"\\t\")[1]\n",
    "            content = content.strip()\n",
    "            results[prefix][key][\"results\"][num] = content\n",
    "        f.close()\n",
    "\n",
    "        gold_file = results[prefix][key]['gold_file']\n",
    "        gf = open(gold_file)\n",
    "        glines=gf.readlines()\n",
    "        results[prefix][key][\"gold_results\"]={}\n",
    "        for gl in glines:\n",
    "            num = int(gl.split(\"\\t\")[0])\n",
    "            content = gl.split(\"\\t\")[1]\n",
    "            content = content.strip()\n",
    "            results[prefix][key][\"gold_results\"][num] = content\n",
    "        gf.close()\n",
    "\n",
    "print(\"done reading the result files\")\n",
    "# Comment this in for inspection of results\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"reading in the bleu-scores\")\n",
    "\n",
    "for prefix in results.keys():\n",
    "    for key in results[prefix].keys():\n",
    "        bleu_file = results[prefix][key][\"bleu_file\"]\n",
    "        f = open(bleu_file)\n",
    "        score=f.readlines()[0]\n",
    "        results[prefix][key][\"bleu\"]=float(score)\n",
    "        f.close()\n",
    "    \n",
    "print(\"done reading the bleu-scores\")\n",
    "\n",
    "#results[\"java\"][\"config_0\"][\"bleu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are little helpers and wrappers to make the notebook a bit smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There is a small issue with the configs being named config_0, config_1, config_10:\n",
    "As they are treated as strings, config_10 is \"smaller then\" config_2, making the sort unintuitive\n",
    "This method should help to sort configs in the intended way: config_1,config_2,...,config_9,config_10,config_11,...\n",
    "\n",
    "config_num can be used to sort the configs where necessary. It can be used e.g. as \n",
    "    sorted(non_reference_configs,key=config_num)\n",
    "\"\"\"\n",
    "def config_num(c):\n",
    "    # Fallback: If we are not trying to sort configs, just do a normal compare\n",
    "    if not \"config_\" in c:\n",
    "        return -1\n",
    "    else:\n",
    "        c_part = int(c.split(\"_\")[1])\n",
    "        return c_part\n",
    "\n",
    "# The non reference configs are all result-keys that are not \"reference\"\n",
    "# Additionally, they are sorted to match the above behaviour (config10>config2)\n",
    "non_reference_configs = [] \n",
    "\n",
    "for prefix in results.keys():\n",
    "    non_reference_configs += sorted([(prefix,k) for k in results[prefix].keys() if \"reference\" != k],key=config_num)\n",
    "\n",
    "print(non_reference_configs)\n",
    "# Set the Archetypes also into the results using the Archetype Dictionary defined at the beginning of the notebook\n",
    "for (prefix, key) in non_reference_configs:\n",
    "        if \"property_file\" in results[prefix][key].keys():\n",
    "            results[prefix][key][\"archetype\"]=config_archetypes[key]\n",
    "\n",
    "# This helps looking up archetype+transformations per configuration\n",
    "def archetype_info(config,prefix=\"java\"):\n",
    "    archetype = config_archetypes[config]\n",
    "    transforms = int(results[prefix][config][\"properties\"][\"transformations\"])\n",
    "    return (archetype,transforms)\n",
    "\n",
    "# Pretty Print archetype info for a given config\n",
    "print_archetype_info = lambda config: f\"{(archetype_info(config))[0]}@{(archetype_info(config))[1]}\"\n",
    "\n",
    "# Another Set of archetypes used e.g. for grouping and printing\n",
    "all_archetypes = set(config_archetypes.values())\n",
    "\n",
    "# archetype-MT-Mapping for Paper (Where we use MT)\n",
    "archetype_mt_mapping = {\n",
    "    \"if\":\"MT-IF\",\n",
    "    \"neutral-element\":\"MT-NE\",\n",
    "    \"mixed names(pseudo)\": \"MT-REP + MT-UVP\",\n",
    "    \"mixed-names(random)\": \"MT-RER + MT-UVR\",\n",
    "    \"add-var(pseudo)\":\"MT-UVP\",\n",
    "    \"add-var(random)\":\"MT-UVR\",\n",
    "    \"if & neutral-element\":\"MT-IF + MT-NE\",\n",
    "    \"lambda\":\"MT-L\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# These Two Wrappers are adapters to the ntlk library, \n",
    "# In addition they cover often-occuring errors with a default behaviour\n",
    "# (Instead of throwing errors)\n",
    "\n",
    "def jaccard_wrapper(sentenceA,sentenceB,ngram=1,lowercasing=True):\n",
    "    a = sentenceA.lower() if lowercasing else sentenceA\n",
    "    b = sentenceB.lower() if lowercasing else sentenceB\n",
    "    tokensA = nltk.word_tokenize(a)\n",
    "    tokensB = nltk.word_tokenize(b)\n",
    "\n",
    "    ngA_tokens = set(nltk.ngrams(tokensA, n=ngram))\n",
    "    ngB_tokens = set(nltk.ngrams(tokensB, n=ngram))\n",
    "    \n",
    "    if (len(ngB_tokens)==0) and (len(ngA_tokens)==0):\n",
    "        return 0\n",
    "    if (len(ngB_tokens)==0) or (len(ngA_tokens)==0):\n",
    "        return 1\n",
    "    \n",
    "    return nltk.jaccard_distance(ngA_tokens, ngB_tokens)\n",
    "\n",
    "def bleu_wrapper(sentence_to_check,reference):\n",
    "    check_tokens = nltk.word_tokenize(sentence_to_check)\n",
    "    ref_tokens = nltk.word_tokenize(reference)\n",
    "    \n",
    "    # From comparing the foreign_bleu and nltk the method4 seems to match\n",
    "    # The Paper names the BLEU-4-Score with a citation to chen & cherry\n",
    "    # I wish I could be named chen & cherry, its a very cool name. \n",
    "    chencherry = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    smooth_fn = chencherry.method4\n",
    "    \n",
    "    try:\n",
    "        return nltk.translate.bleu_score.sentence_bleu([ref_tokens],check_tokens,smoothing_function=smooth_fn)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bleu-Scores\n",
    "\n",
    "In the following, the BLEU-scores will be calculated using the foreign libary. \n",
    "While there have been minor changes to standard-BLEU, it is the same as used in the original experiment.\n",
    "\n",
    "The aggregated BLEU-Scores will be stored to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bleu_data = {}\n",
    "for prefix in results.keys():\n",
    "    archetypes = set([results[prefix][k][\"archetype\"] for k in results[prefix].keys() if \"archetype\" in results[prefix][k].keys()])\n",
    "    for archetype in archetypes:\n",
    "        bleu_data[archetype]={}\n",
    "        bleu_data[archetype][0]=results[prefix][\"reference\"][\"bleu\"]\n",
    "        relevant_configs = [k for k \n",
    "                            in results[prefix].keys() \n",
    "                            if \"archetype\" in results[prefix][k].keys() \n",
    "                            and results[prefix][k][\"archetype\"]==archetype]\n",
    "        for c in relevant_configs:\n",
    "            bleu_data[archetype][int(results[prefix][c][\"properties\"][\"transformations\"])]=results[prefix][c][\"bleu\"]\n",
    "\n",
    "    bleu_data_df = pd.DataFrame.from_dict(bleu_data)\n",
    "    bleu_data_df = bleu_data_df.sort_index()\n",
    "    bleu_data_df = bleu_data_df.applymap(lambda cell: round(cell,3))\n",
    "    bleu_data_df.columns = [archetype_mt_mapping[n] for n in bleu_data_df.columns]\n",
    "\n",
    "    with open(f\"./exports/{prefix}_bleu_table.tex\",\"w\") as f: \n",
    "        f.write(\n",
    "            bleu_data_df.to_latex(\n",
    "                caption=\"BLEU4-Scores for increasing number of metamorphic transformations \\n (applied n-times per datapoint)\"\n",
    "                ,label=\"tab:bleus\"\n",
    "                ,position=\"tbh\"\n",
    "                #,column_format={rrrrrrr}\n",
    "            )         \n",
    "        )\n",
    "\n",
    "bleu_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#bleu_data_df.columns = [archetype_mt_mapping[a] for a in bleu_data_df.columns]\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.ylabel(\"BLEU-Score\",fontsize=20)\n",
    "#plt.xlabel(\"# Transformations\")\n",
    "plt.xlabel(\"Order\",fontsize=22)\n",
    "#for latex, its nicer to have the title set from latex itself\n",
    "#plt.title(\"BLEU4-Scores for increasing number of metamorphic transformations \\n (applied n-times per datapoint)\")\n",
    "\n",
    "plot = sns.lineplot(data=bleu_data_df,markers=True,style=None,dashes=False)\n",
    "plt.xticks([0,1,5,10],fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlim(-0.025,10.1)\n",
    "plt.legend(bleu_data_df.columns,fontsize=16)\n",
    "plt.savefig('images/bleu_scores.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "bleu_data_df_transposed = bleu_data_df.transpose()\n",
    "bleu_data_df_transposed = bleu_data_df_transposed.drop(axis=1,columns=0)\n",
    "with open(\"./exports/transposed_bleu_table.tex\",\"w\") as f: \n",
    "    f.write(\n",
    "        bleu_data_df_transposed.to_latex(\n",
    "            caption=\"BLEU4-Scores for increasing order of metamorphic transformations \\n (applied n-times per datapoint)\"\n",
    "            ,label=\"tab:bleus\"\n",
    "            ,position=\"th\"\n",
    "            #,column_format={rrrrrrr}\n",
    "        )          \n",
    ")\n",
    "\n",
    "#bleu_data_df_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Entry Bleu\n",
    "\n",
    "Now we use the nltk-provided bleu score to calculate the bleu-scores for all entries.\n",
    "We store them on a per-result basis always bleu(gold,config).\n",
    "\n",
    "The nltk bleu does not go from 0 to 100 but from 0 to 1, but they are the same by a factor of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This wrapper applies the \"bleu_wrapper\" to every element of a configurations results.\n",
    "# The result is a list of [bleu-score(config[i],gold[i])]\n",
    "# Entries are in order ascending\n",
    "def calculate_bleus(results,prefix,config_id):\n",
    "    bleus = []\n",
    "    entries = results[prefix][config_id][\"results\"].keys()\n",
    "    print(f\"Calculating bleus for {prefix}-{config_id} ({len(entries)} entries)\")\n",
    "    for i in entries:\n",
    "        gold_result = results[prefix][config_id][\"gold_results\"][i]\n",
    "        config_result = results[prefix][config_id][\"results\"][i]\n",
    "        bleu = bleu_wrapper(gold_result,config_result)\n",
    "        bleus.append(bleu)\n",
    "    return bleus\n",
    "        \n",
    "# Comment in for test and inspection\n",
    "# calculate_bleus(results,\"java\",\"config_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These plots, while not necessary the best, try to compare the bleus of the reference to the bleus of a config.\n",
    "They don't take very long, the actual bleu-calculation is what takes time in the cell below.\n",
    "\"\"\"\n",
    "\n",
    "def plot_bleu_histogram(config_data,reference_data,title):\n",
    "    plt.figure(figsize=(14,7))\n",
    "    \n",
    "    histo_df=pd.DataFrame.from_dict(\n",
    "        {\"reference\":reference_data,\n",
    "            title:config_data }\n",
    "    )\n",
    "\n",
    "    sns.displot(\n",
    "        data=histo_df,\n",
    "        kind=\"hist\", kde=True,\n",
    "        height=6, aspect=10/6\n",
    "               )\n",
    "    plt.title(f\"Histogram of Bleu-Scores for {title}\")\n",
    "    plt.xlabel(\"Bleu-Score\")\n",
    "    #plt.ylabel(\"# of Entries\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.savefig(f'images/{title}_bleu_histogram.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_bleu_boxplot(config_data,reference_data,title=None):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    box_df=pd.DataFrame.from_dict(\n",
    "        {\"reference\":reference_data,\n",
    "            title:config_data }\n",
    "    )    \n",
    "    sns.boxplot(\n",
    "        data=box_df)\n",
    "    \n",
    "    plt.title(f\"Boxplot of Bleu-Scores for {title}\")\n",
    "    plt.ylabel(\"Bleu-Score\")\n",
    "    \n",
    "    major_ticks = np.arange(0, 1, 0.2)\n",
    "    minor_ticks = np.arange(0, 1, 0.05)\n",
    "\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "\n",
    "    # And a corresponding grid\n",
    "    ax.grid(which='both')\n",
    "\n",
    "    #plt.grid()\n",
    "    plt.savefig(f'images/{title}_bleu_box.png')\n",
    "    plt.ylim(0,1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_bleu_violinplot(config_data,reference_data,title):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    print(len(reference_data),len(config_data))\n",
    "    violin_df=pd.DataFrame.from_dict(\n",
    "        {\"reference\":reference_data,\n",
    "            title:config_data }\n",
    "    )\n",
    "    \n",
    "    sns.violinplot(data=violin_df)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.title(f\"ViolinPlot of Bleu-Scores for {title}\")\n",
    "    plt.ylabel(\"Bleu-Score\")\n",
    "    \n",
    "    plt.savefig(f'images/{title}_bleu_violin.png')\n",
    "    plt.show()\n",
    "    \n",
    "#plot_bleu_violinplot(sample_bleus_config_data,bleus_reference_data,\"config_20\")\n",
    "#plot_bleu_boxplot(sample_bleus_config_data,bleus_reference_data,\"config_20\")\n",
    "#plot_bleu_histogram(sample_bleus_config_data,bleus_reference_data,\"config_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# For every entry in the config, calculate bleu and make comparison plots\n",
    "for prefix in results.keys():\n",
    "    # Calculate the reference bleus and store them\n",
    "    bleus_reference_data = calculate_bleus(results,prefix,\"reference\")\n",
    "    results[prefix][\"reference\"][\"bleu_values\"]=bleus_reference_data\n",
    "    for (p,conf) in non_reference_configs:\n",
    "        print(prefix,conf)\n",
    "        print()\n",
    "        bleus_data = calculate_bleus(results,prefix,conf)\n",
    "        # Set the bleu values to only calculate them once\n",
    "        results[prefix][conf][\"bleu_values\"]=bleus_data\n",
    "        # Use the bleu-data to make some plots\n",
    "        plot_bleu_violinplot(bleus_data,bleus_reference_data,f\"{prefix}-{conf}\")\n",
    "        plot_bleu_boxplot(bleus_data,bleus_reference_data,f\"{prefix}-{conf}\")\n",
    "        plot_bleu_histogram(bleus_data,bleus_reference_data,f\"{prefix}-{conf}\")\n",
    "        # Delete the bleu data to free some memory and not collide on names\n",
    "        del bleus_data\n",
    "    del bleus_reference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "Before the samples can be inspected, the items need to be re-indexed. \n",
    "While all config_results are in the reference_results, there might is an issue with the data being shuffeld. \n",
    "\n",
    "To fix this, a reindexing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Reindexing Pseudocode\n",
    "\n",
    "def lookup_index(sentence, comparison_dict):\n",
    "    for (gold_key,gold_value) in comparison_dict.items():\n",
    "        if sentence == gold_value:\n",
    "            return gold_key\n",
    "    return -1\n",
    "\n",
    "# Pseudocode:\n",
    "# For each config (that is not reference)\n",
    "    # Create a lookup of reference_gold_index -> config_gold_index\n",
    "    # Invert the lookup \n",
    "    # Make a new dictionary where\n",
    "        # For every key of the config_gold\n",
    "        # lookup the key of the reference_gold\n",
    "        # And fill it with {reference_gold_key : config_gold_value}\n",
    "        # Do the same with the non-gold results\n",
    "        # Fill it with {reference_gold_key : config_result_value}\n",
    "    # Set result[config_X][\"gold_results\"] to the newly created, matching index one \n",
    "    # same for non-gold-results\n",
    "\n",
    "for (prefix,config) in non_reference_configs:\n",
    "    keyMapping={}\n",
    "    for (k,v) in results[prefix][config][\"gold_results\"].items():\n",
    "        gk = lookup_index(v,results[prefix][\"reference\"][\"gold_results\"])\n",
    "        keyMapping[k]=gk\n",
    "    new_gold_results={}\n",
    "    new_results={}\n",
    "    for (config_key,gold_key) in keyMapping.items():\n",
    "        if gold_key != -1:\n",
    "            new_gold_results[gold_key]=results[prefix][config][\"gold_results\"][config_key]\n",
    "            new_results[gold_key]=results[prefix][config][\"results\"][config_key]\n",
    "    results[prefix][config][\"gold_results\"]=new_gold_results\n",
    "    results[prefix][config][\"results\"]=new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short Example that the reindexing worked and looks about right\n",
    "sample_index = 250\n",
    "print(results[\"java\"][\"reference\"][\"gold_results\"][sample_index] )\n",
    "print()\n",
    "print(results[\"java\"][\"reference\"][\"results\"][sample_index])\n",
    "print(results[\"java\"][\"config_2\"][\"results\"][sample_index])\n",
    "del sample_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing and Sampling\n",
    "\n",
    "These cells look into the entries and find outstanding / most prominent results given diverse criteria. \n",
    "As they are qualitative inspections, they are not being plotted but only printed.\n",
    "\n",
    "(Previously *hall of shame*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "biggest_len_inc = 0\n",
    "biggest_len_inc_pos = ()\n",
    "\n",
    "biggest_len_dec = 0\n",
    "biggest_len_dec_pos = ()\n",
    "\n",
    "biggest_jaccard_dist = 0\n",
    "biggest_jaccard_dist_pos = ()\n",
    "\n",
    "smallest_jaccard_dist = 1 \n",
    "smallest_jaccard_dist_pos = ()\n",
    "\n",
    "for (prefix,config) in non_reference_configs:\n",
    "    for index in list(results[prefix][config][\"results\"].keys()):\n",
    "        gold = results[prefix][\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[prefix][\"reference\"][\"results\"][index]\n",
    "        altered = results[prefix][config][\"results\"][index]\n",
    "        \n",
    "        if len(reference)-len(altered)>biggest_len_inc:\n",
    "            biggest_len_inc = len(reference)-len(altered)\n",
    "            biggest_len_inc_pos = (index,prefix,config)\n",
    "        if len(altered)-len(reference)>biggest_len_dec:\n",
    "            biggest_len_dec = len(altered)-len(reference)\n",
    "            biggest_len_dec_pos = (index,prefix,config)\n",
    "            \n",
    "        jacc_dist = jaccard_wrapper(altered,reference)\n",
    "        if jacc_dist > biggest_jaccard_dist and jacc_dist < 1:\n",
    "            biggest_jaccard_dist = jacc_dist\n",
    "            biggest_jaccard_dist_pos = (index,prefix,config)\n",
    "        if jacc_dist < smallest_jaccard_dist and jacc_dist > 0:\n",
    "            smallest_jaccard_dist = jacc_dist\n",
    "            smallest_jaccard_dist_pos = (index,prefix,config)\n",
    "            \n",
    "# This method prints the i'ths entry of config X aswell as the gold and reference entry for it.\n",
    "def print_config_item_with_reference(index,prefix,config):\n",
    "    print(\"Gold:\")\n",
    "    print(results[prefix][config][\"gold_results\"][index])\n",
    "    print(\"Reference:\")\n",
    "    print(results[prefix][\"reference\"][\"results\"][index])\n",
    "    print(f\"Altered ({prefix}:{config}@{index}):\")\n",
    "    print(results[prefix][config][\"results\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Biggest jaccard Distance (that is not 1):\\n\")\n",
    "print_config_item_with_reference(biggest_jaccard_dist_pos[0],biggest_jaccard_dist_pos[1],biggest_jaccard_dist_pos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Biggest decrease in length:\\n\")\n",
    "print_config_item_with_reference(biggest_len_inc_pos[0],biggest_len_inc_pos[1],biggest_len_inc_pos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Biggest increase in length:\\n\")\n",
    "print_config_item_with_reference(biggest_len_dec_pos[0],biggest_len_dec_pos[1],biggest_len_dec_pos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Smallest Jaccard Distance (that is not 0):\\n\")\n",
    "print_config_item_with_reference(smallest_jaccard_dist_pos[0],smallest_jaccard_dist_pos[1],smallest_jaccard_dist_pos[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fishy Example from a Kids Java-Learning Book.** \n",
    "Code is actually about learning switch-case statements and set a image to the corresponding fishes (e.g. empty fish glass, fish glass with 2 fishes etc.)\n",
    "\n",
    "The code examples are put into the paper repository as a separate artefact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishyKey = -1\n",
    "for (key,value) in results[\"java\"][\"reference\"][\"gold_results\"].items():\n",
    "    #print(value)\n",
    "    if \"makeAFishyDecision \" in value:\n",
    "        fishyKey = key\n",
    "\n",
    "print(\"Fishy Results! \\n\")\n",
    "print(\"Gold:\")\n",
    "print(results[\"java\"][\"reference\"][\"gold_results\"][fishyKey])\n",
    "print(\"Reference:\")\n",
    "print(results[\"java\"][\"reference\"][\"results\"][fishyKey],\"\\n\")\n",
    "#for (prefix,config) in non_reference_configs:\n",
    "for config in [\"config_0\",\"config_1\",\"config_2\",\"config_5\",\"config_9\"]:\n",
    "    print(f\"Altered(java:{config},{print_archetype_info(config)}):\")\n",
    "    print(results[\"java\"][config][\"results\"][fishyKey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries_to_look_at = 3\n",
    "prefix_to_look_at = \"java\"\n",
    "longest_gold = sorted(list(results[prefix_to_look_at][\"reference\"][\"gold_results\"].items()),reverse=True,key=lambda pair: len(pair[1]))[:entries_to_look_at]\n",
    "#longest_gold\n",
    "for l_gold in longest_gold:\n",
    "    #for config in non_reference_configs:\n",
    "    for config in [\"config_1\",\"config_7\",\"config_14\"]:\n",
    "        print_config_item_with_reference(l_gold[0],prefix_to_look_at,config)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shortest_gold = sorted(list(results[prefix_to_look_at][\"reference\"][\"gold_results\"].items()),reverse=True,key=lambda pair: len(pair[1]))[-3:]\n",
    "#shortest_gold\n",
    "for s_gold in shortest_gold:\n",
    "    #for config in non_reference_configs:\n",
    "    for config in [\"config_1\",\"config_7\",\"config_14\"]:\n",
    "        print_config_item_with_reference(s_gold[0],prefix_to_look_at,config)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the shortest gold standard you can clearly see that the gold-standard is cut at the first @-Sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for certain key-words in the altered configs\n",
    "\n",
    "We want to inspect \n",
    "\n",
    "- where is the keyword x the most times\n",
    "- how often does keyword x appear in results for config x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entry_with_most_frequent_keyword(keyword):\n",
    "    most_keywords=0\n",
    "    most_keywords_pos=()\n",
    "\n",
    "    for (prefix,config) in non_reference_configs:\n",
    "        for index in list(results[prefix][config][\"results\"].keys()):\n",
    "            altered = results[prefix][config][\"results\"][index]\n",
    "\n",
    "            keywords = altered.lower().count(keyword)\n",
    "            if keywords>most_keywords:\n",
    "                most_keywords = keywords\n",
    "                most_keywords_pos = (index,prefix,config)\n",
    "    return most_keywords_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_adds = find_entry_with_most_frequent_keyword(\"add\")\n",
    "print(f\"Most occurrences of 'add':\\n\")\n",
    "print_config_item_with_reference(most_adds[0],most_adds[1],most_adds[2])\n",
    "print()\n",
    "\n",
    "most_gets = find_entry_with_most_frequent_keyword(\"get\")\n",
    "print(f\"Most occurrences of 'get':\\n\")\n",
    "print_config_item_with_reference(most_gets[0],most_gets[1],most_gets[2])\n",
    "print()\n",
    "\n",
    "most_configs = find_entry_with_most_frequent_keyword(\"config\")\n",
    "print(f\"Most occurrences of 'config':\\n\")\n",
    "print_config_item_with_reference(most_configs[0],most_configs[1],most_configs[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "looks for a certain keyword in the results.\n",
    "If a config is specified, it only tries to look for that config.\n",
    "Searches in all configs otherwise.\n",
    "Returns the entries containing the keyword as a list of pairs (index,config)\n",
    "\"\"\"\n",
    "def find_entries_with_keyword(keyword,config=None,prefix=None):\n",
    "    entries=[]\n",
    "\n",
    "    if config and prefix:\n",
    "        for index in list(results[prefix][config][\"results\"].keys()):\n",
    "                altered = results[prefix][config][\"results\"][index]\n",
    "                if keyword in altered.lower():\n",
    "                    entries.append((index,prefix,config)) \n",
    "    else:    \n",
    "        for (prefix,config) in non_reference_configs:\n",
    "            for index in list(results[prefix][config][\"results\"].keys()):\n",
    "                altered = results[prefix][config][\"results\"][index]\n",
    "                if keyword in altered.lower():\n",
    "                    entries.append((index,prefix,config))\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Altered-Entries with 'add':\\t{len(find_entries_with_keyword('add'))}\")\n",
    "print(f\"Altered-Entries with 'get':\\t{len(find_entries_with_keyword('get'))}\")\n",
    "print(f\"Altered-Entries with 'get':\\t{len(find_entries_with_keyword('set'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The configs 6 7 and 8 are the \"Add Neutral\" Transformations\n",
    "print(f\"Entries with 'add' in 'java:reference':\\t{len(find_entries_with_keyword('add','reference','java'))}\")\n",
    "\n",
    "print(f\"Entries with 'add' in 'java:config_6':\\t{len(find_entries_with_keyword('add','config_6','java'))}\")\n",
    "print(f\"Entries with 'add' in 'java:config_7':\\t{len(find_entries_with_keyword('add','config_7','java'))}\")\n",
    "print(f\"Entries with 'add' in 'java:config_8':\\t{len(find_entries_with_keyword('add','config_8','java'))}\")\n",
    "print()\n",
    "\n",
    "keyword=\"mock\"\n",
    "for (prefix,config) in non_reference_configs:\n",
    "    print(f\"Entries with '{keyword}' in '{prefix}:{config}':\\t{len(find_entries_with_keyword(keyword,config,prefix))}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no significant change in what are \"getters\",\"setters\" and similar items. \n",
    "They appear mostly evenly distributed and staying that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differences in AddVar5 to AddVar10**\n",
    "\n",
    "Next examples look into bleu differences and \"why\" addvar10 full random is doing better than addvar5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Config 13 and 14 are add_var(5,random) and add_var(10,random)\n",
    "# Do this for Java only at the moment\n",
    "\n",
    "add_var_diffs = []\n",
    "prefix_to_look_at = \"java\"\n",
    "\n",
    "for index in list(results[prefix_to_look_at][\"config_13\"][\"results\"].keys()):\n",
    "    addvar5result=results[prefix_to_look_at][\"config_13\"][\"results\"][index]\n",
    "    addvar10result=results[prefix_to_look_at][\"config_14\"][\"results\"][index]\n",
    "    reference=results[prefix_to_look_at][\"reference\"][\"results\"][index]\n",
    "    gold=results[prefix_to_look_at][\"reference\"][\"gold_results\"][index]\n",
    "    \n",
    "    addvar5bleu = bleu_wrapper(addvar5result,gold)\n",
    "    addvar10bleu = bleu_wrapper(addvar10result,gold)\n",
    "    diff = (addvar5bleu-addvar10bleu,index)\n",
    "    add_var_diffs.append(diff)\n",
    "    \n",
    "add_var_diffs=sorted(add_var_diffs,key=lambda p:p[0])\n",
    "\n",
    "for worsties in add_var_diffs[-5:]:\n",
    "    print(f\"Worsened bleu by {worsties[0]}\")\n",
    "    print(\"Gold:\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['reference']['gold_results'][worsties[1]]}\")\n",
    "    print(\"Reference:\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['reference']['results'][worsties[1]]}\")\n",
    "    print(\"AddVar+RenameParam(5):\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['config_13']['results'][worsties[1]]}\")\n",
    "    print(\"AddVar+RenameParam(10):\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['config_14']['results'][worsties[1]]}\")\n",
    "    print()\n",
    "\n",
    "for besties in add_var_diffs[:5]:\n",
    "    print(f\"Bettered bleu by {besties[0]}\")\n",
    "    print(\"Gold:\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['reference']['gold_results'][besties[1]]}\")\n",
    "    print(\"Reference:\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['reference']['results'][besties[1]]}\")\n",
    "    print(\"AddVar+RenameParam(5):\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['config_13']['results'][besties[1]]}\")\n",
    "    print(\"AddVar+RenameParam(10):\")\n",
    "    print(f\"\\t{results[prefix_to_look_at]['config_14']['results'][besties[1]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However these are not helpfull, they only show that the biggest bleu movements are in getters and setters, which is the same behaviour than in other non addvar-entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Distances\n",
    "\n",
    "The following cells want to inspect the jaccard distances. \n",
    "\n",
    "For now, I looked mostly into jaccard(config,reference), but the same plots can be re-done for jaccard(config,gold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method requires for the xs and ys to be sorted! \n",
    "Without matching indizes it does not make any sense.\n",
    "\"\"\"\n",
    "def calculate_jaccard_distances(xs,ys,ngrams=1):\n",
    "    agg = []\n",
    "    indX = len(xs)\n",
    "    indY = len(ys)\n",
    "    if indX != indY:\n",
    "        raise IndexError()\n",
    "    else:\n",
    "        running_index = 0\n",
    "        while running_index < indX:\n",
    "            agg.append(jaccard_wrapper(xs[running_index],ys[running_index],ngrams))\n",
    "            running_index = running_index + 1\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jaccs = {}\n",
    "for (prefix,_) in non_reference_configs:\n",
    "    jaccs[prefix] = {}\n",
    "    \n",
    "for (prefix,config) in non_reference_configs:\n",
    "    distances = calculate_jaccard_distances(results[prefix][\"reference\"][\"results\"],results[prefix][config][\"results\"])\n",
    "    \n",
    "    jaccs[prefix][config]=distances\n",
    "    plt.figure(figsize=(20,12))\n",
    "    sns.displot(\n",
    "        distances,\n",
    "        kind=\"hist\", kde=True,\n",
    "        bins=20\n",
    "    )\n",
    "    plt.title(f\"Histogram of JaccardDistances for {prefix}:{config}\\n({print_archetype_info(config)})\")\n",
    "    plt.xlabel(\"JaccardDistance \\n Reference to Altered\")\n",
    "    plt.ylabel(\"# of Entries\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,10000)\n",
    "    \n",
    "    plt.savefig(f'images/{prefix}_{config}_jaccard_histogram.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccs_n2 = {}\n",
    "for (prefix,_) in non_reference_configs:\n",
    "    jaccs_n2[prefix] = {}\n",
    "    \n",
    "for (prefix,config) in non_reference_configs:\n",
    "    distances = calculate_jaccard_distances(results[prefix][\"reference\"][\"results\"],results[prefix][config][\"results\"],ngrams=2)\n",
    "    \n",
    "    jaccs_n2[prefix][config]=distances\n",
    "    plt.figure(figsize=(20,12))\n",
    "    sns.displot(\n",
    "        distances,\n",
    "        kind=\"hist\", kde=True,\n",
    "        bins=20\n",
    "    )\n",
    "    plt.title(f\"Histogram of JaccardDistances for {prefix}:{config}\\n({print_archetype_info(config)})\")\n",
    "    plt.xlabel(\"JaccardDistance (ngram=2) \\n Reference to Altered\")\n",
    "    plt.ylabel(\"# of Entries\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,10000)\n",
    "    \n",
    "    plt.savefig(f'images/{prefix}_{config}_jaccard_ngram2_histogram.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacc_data = []\n",
    "for prefix in jaccs.keys():\n",
    "    for config in jaccs[prefix].keys():\n",
    "        jacc_data.append((prefix,config,config_archetypes[config],jaccs[prefix][config]))\n",
    "\n",
    "df = pd.DataFrame(jacc_data)\n",
    "df.columns=[\"prefix\",\"config\",\"archetype\",\"jacc_dist\"]\n",
    "df = df.explode('jacc_dist')\n",
    "df['jacc_dist'] = df['jacc_dist'].astype('float')\n",
    "df = df.dropna()\n",
    "\n",
    "plt.figure(figsize=(30,12))\n",
    "\n",
    "sns.boxplot(\n",
    "    x=\"config\",\n",
    "    y=\"jacc_dist\",\n",
    "    hue=\"archetype\",\n",
    "    #width=4.5,\n",
    "    dodge =False,\n",
    "    data=df)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"Boxplot of Jaccard_Distances\")\n",
    "plt.ylabel(\"Jaccard Distance\")\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(f'images/jaccard_distances_boxplot.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,12))\n",
    "sns.violinplot(\n",
    "    x=\"config\",\n",
    "    hue=\"archetype\",\n",
    "    y=\"jacc_dist\",\n",
    "    data=df,\n",
    "#width=5.5,\n",
    "  showmeans=False,\n",
    "  showmedians=False,\n",
    "    inner=None,\n",
    "    dropnan=True,\n",
    "    dropna=True,\n",
    "    dodge =False\n",
    ")\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(f'images/jaccard_distances_violinplot.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "This is a different approach to gather all data in a pandas frame and then make 3 dimensional plots and other funny things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Driver for the time is the jaccard distance \n",
    "\n",
    "result_df_data = []\n",
    "\n",
    "for (prefix,config) in non_reference_configs:\n",
    "    arch = config_archetypes[config]\n",
    "    ts = results[prefix][config][\"properties\"][\"transformations\"]\n",
    "    index = 0\n",
    "    while index < len(results[prefix][config][\"results\"]):\n",
    "        ref = results[prefix][\"reference\"][\"results\"][index]\n",
    "        res = results[prefix][config][\"results\"][index]\n",
    "        gold = results[prefix][\"reference\"][\"gold_results\"][index]\n",
    "        bleu = results[prefix][config][\"bleu_values\"][index]\n",
    "        ref_bleu = results[prefix][\"reference\"][\"bleu_values\"][index]\n",
    "        \n",
    "        diff = res != ref\n",
    "        perfect = gold == res\n",
    "        \n",
    "        # Distance Gold<>ConfigText\n",
    "        jacc_1 = jaccard_wrapper(res,gold,ngram=1)\n",
    "        jacc_2 = jaccard_wrapper(res,gold,ngram=2)\n",
    "        # Distance Gold<>ReferenceText\n",
    "        jacc_1_ref = jaccard_wrapper(ref,res,ngram=1)\n",
    "        jacc_2_ref = jaccard_wrapper(ref,res,ngram=2)\n",
    "        \n",
    "        result_df_data.append(\n",
    "            (prefix,config,arch,archetype_mt_mapping[arch],ts,index,\n",
    "             bleu,ref_bleu,\n",
    "             diff,perfect,\n",
    "             jacc_1,jacc_2,\n",
    "             jacc_1_ref,jacc_2_ref,\n",
    "             gold,ref,res)\n",
    "        )\n",
    "        index = index + 1\n",
    "\n",
    "result_df = pd.DataFrame(result_df_data)\n",
    "\n",
    "result_df.columns=[\n",
    "    \"prefix\",\"config\",\"archetype\",\"MT\",\"transformations\",\"index\",\n",
    "    \"bleu\",\"reference_bleu\",\n",
    "    \"difference\",\"perfect_match\",\n",
    "    \"jaccard_n1\",\"jaccard_n2\",\"jaccard_n1_reference\",\"jaccard_n2_reference\",\n",
    "    \"gold_result\",\"reference_result\",\"config_result\"\n",
    "]\n",
    "\n",
    "#result_df = result_df.dropna()\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences\n",
    "\n",
    "Looking for Differences in results - similar to Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 7))\n",
    "plt.grid()\n",
    "\n",
    "plt.title('Result-Differences per Configuration')\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "    x=\"config\",y=\"difference\",\n",
    "    data=result_df,\n",
    "    hue=\"MT\",\n",
    "    dodge =False\n",
    ")\n",
    "\n",
    "\n",
    "plt.savefig(f'images/number_of_diffs_by_config.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ2 Results\n",
    "For RQ2 we first needed to have simple counts and percentages of the mere numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"java\",\"python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for lan in languages:\n",
    "    totalPerO = result_df[(result_df[\"transformations\"]=='1') & (result_df[\"prefix\"]==lan)].count()[0]\n",
    "    firstOdiff = result_df[(result_df[\"transformations\"]=='1') & (result_df[\"difference\"]) & (result_df[\"prefix\"]==lan)].count()[0]\n",
    "    fifthOdiff = result_df[(result_df[\"transformations\"]=='5') & (result_df[\"difference\"]) & (result_df[\"prefix\"]==lan)].count()[0]\n",
    "    tenthOdiff = result_df[(result_df[\"transformations\"]=='10') & (result_df[\"difference\"]) & (result_df[\"prefix\"]==lan)].count()[0]\n",
    "    \n",
    "    print(lan)\n",
    "    print(\"Total number of entries per Order:\",totalPerO)\n",
    "    print(f\"Changes in first order {firstOdiff}({round(firstOdiff/totalPerO,3)}%)\")\n",
    "    print(f\"Changes in fifth order {fifthOdiff}({round(fifthOdiff/totalPerO,3)}%)\")\n",
    "    print(f\"Changes in tenth order {tenthOdiff}({round(tenthOdiff/totalPerO,3)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ1 Results\n",
    "These are some infos on the changed and affected results for the first order mts changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_df = result_df.copy()\n",
    "plot_df = plot_df[plot_df[\"transformations\"]=='1']\n",
    "\n",
    "plot_df[\"jacc1_diff\"] = plot_df[\"jaccard_n1\"]-plot_df[\"jaccard_n1_reference\"]\n",
    "plot_df[\"abs_jacc1_diff\"] = abs(plot_df[\"jacc1_diff\"])\n",
    "plot_df[\"bleu_diff\"] = plot_df[\"bleu\"]-plot_df[\"reference_bleu\"]\n",
    "plot_df[\"abs_bleu_diff\"]=abs(plot_df[\"bleu_diff\"])\n",
    "\n",
    "diffed_df = plot_df[plot_df[\"jaccard_n1_reference\"]>0]\n",
    "\n",
    "plot_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    print(lan)\n",
    "    post_1stMT_count = plot_df[plot_df[\"prefix\"]==lan].count()[0]\n",
    "    count_jacc_samsies = plot_df[(plot_df[\"jaccard_n1_reference\"]==0)  & (plot_df[\"prefix\"]==lan)].count()[0]\n",
    "    count_jacc_diffs = diffed_df[diffed_df[\"prefix\"]==lan].count()[0]\n",
    "    count_bleu_diffs = plot_df[(plot_df[\"abs_bleu_diff\"]>0) & (plot_df[\"prefix\"]==lan)].count()[0]\n",
    "\n",
    "    avg_bleu_diff = np.mean(plot_df[plot_df[\"abs_bleu_diff\"]>0 & (plot_df[\"prefix\"]==lan)][\"abs_bleu_diff\"])\n",
    "\n",
    "    print(\"Entries for first order\",post_1stMT_count)\n",
    "    print(\"Jaccard Changes:\",count_jacc_diffs)\n",
    "    print(f\"BLEU Changes: {count_bleu_diffs}({round(count_bleu_diffs/post_1stMT_count,3)}%)\")\n",
    "    print(\"Average Bleu-Diff:\",avg_bleu_diff)\n",
    "\n",
    "    avg_jacc_diff = np.mean(plot_df[(plot_df[\"abs_bleu_diff\"]>0) & (plot_df[\"prefix\"]==lan) ][\"abs_bleu_diff\"])\n",
    "    median_jacc_diff = np.median(plot_df[plot_df[\"prefix\"]==lan][\"jaccard_n1_reference\"])\n",
    "    iqr_jacc_diff = stats.iqr(plot_df[plot_df[\"prefix\"]==lan][\"jaccard_n1_reference\"])\n",
    "\n",
    "    print(\"Average Jacc Diff:\",avg_jacc_diff)\n",
    "    print(\"Median Jacc Diff:\",median_jacc_diff)\n",
    "    print(\"IQR Jacc Diffs:\",iqr_jacc_diff)\n",
    "    print()\n",
    "    del count_jacc_samsies,count_jacc_diffs,count_bleu_diffs,avg_bleu_diff,avg_jacc_diff, median_jacc_diff, iqr_jacc_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of changes \n",
    "(To show nice non-null changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16.5, 8.8))\n",
    "\n",
    "sns.histplot(ax=axes[0],data=plot_df,\n",
    "             x=\"jaccard_n1_reference\",\n",
    "             bins=25)\n",
    "axes[0].set(xlim=(0,1.01))\n",
    "\n",
    "axes[0].set_xlabel(f'Difference in Jaccard Distance \\n Reference <> First Order MTs', fontsize=23)\n",
    "axes[0].set_ylabel('Number of Entries', fontsize=19)\n",
    "\n",
    "axes[0].set_xticks(np.arange(0,1.2,0.2))\n",
    "axes[0].set_xticklabels([round(x,1) for x in np.arange(0,1.2,0.2)],fontsize=17)\n",
    "\n",
    "axes[0].set_yticklabels([int(a) for a in axes[0].get_yticks()],fontsize=17)\n",
    "\n",
    "sns.histplot(ax=axes[1],data=diffed_df,\n",
    "             x=\"abs_bleu_diff\",\n",
    "             bins=50)\n",
    "\n",
    "axes[1].set(xlim=(0,1.01))\n",
    "axes[1].set_xlabel(f'Absolute Difference in BLEU4-Score \\n for Summaries with Jaccard-Delta', fontsize=23)\n",
    "axes[1].set_ylabel('Number of Entries', fontsize=19)\n",
    "\n",
    "axes[1].set_xticks(np.arange(0,1.2,0.2))\n",
    "axes[1].set_xticklabels([round(x,1) for x in np.arange(0,1.2,0.2)],fontsize=17)\n",
    "\n",
    "axes[1].set_yticklabels([int(a) for a in axes[1].get_yticks()],fontsize=17)\n",
    "\n",
    "plt.savefig(f'images/overview_plot_changes_of_firstorder_mts_small_combined.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16.5, 8.8))\n",
    "\n",
    "    sns.histplot(ax=axes[0],data=plot_df[plot_df[\"prefix\"]==lan],\n",
    "                 x=\"jaccard_n1_reference\",\n",
    "                 bins=25)\n",
    "    axes[0].set(xlim=(0,1.01))\n",
    "\n",
    "    axes[0].set_xlabel(f'Difference in Jaccard Distance \\n Reference <> First Order MTs in {lan}', fontsize=23)\n",
    "    axes[0].set_ylabel('Number of Entries', fontsize=19)\n",
    "\n",
    "    axes[0].set_xticks(np.arange(0,1.2,0.2))\n",
    "    axes[0].set_xticklabels([round(x,1) for x in np.arange(0,1.2,0.2)],fontsize=17)\n",
    "\n",
    "    axes[0].set_yticklabels([int(a) for a in axes[0].get_yticks()],fontsize=17)\n",
    "\n",
    "    sns.histplot(ax=axes[1],data=diffed_df,\n",
    "                 x=\"abs_bleu_diff\",\n",
    "                 bins=50)\n",
    "\n",
    "    axes[1].set(xlim=(0,1.01))\n",
    "    axes[1].set_xlabel(f'Absolute Difference in BLEU4-Score \\n for Summaries with Jaccard-Delta in {lan}', fontsize=23)\n",
    "    axes[1].set_ylabel('Number of Entries', fontsize=19)\n",
    "\n",
    "    axes[1].set_xticks(np.arange(0,1.2,0.2))\n",
    "    axes[1].set_xticklabels([round(x,1) for x in np.arange(0,1.2,0.2)],fontsize=17)\n",
    "\n",
    "    axes[1].set_yticklabels([int(a) for a in axes[1].get_yticks()],fontsize=17)\n",
    "\n",
    "    plt.savefig(f'images/overview_plot_changes_of_firstorder_mts_small_{lan}.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#del plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Example for a where the MT-IF creates \"growth\" in the result\n",
    "result_df[(result_df[\"MT\"]==\"MT-IF\") & (result_df[\"index\"]==327)][\"reference_bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 7))\n",
    "plt.grid()\n",
    "\n",
    "sns.scatterplot(x=\"jaccard_n1\",y=\"bleu\",hue=\"config\",style=\"archetype\",data=result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.grid()\n",
    "\n",
    "plt.title(\"Scatterplot of Entries \\n Bleu<>ReferenceBleu\")\n",
    "\n",
    "sns.scatterplot(x=\"reference_bleu\",y=\"bleu\",hue=\"config\",size=\"jaccard_n1\",style=\"archetype\",data=result_df[result_df.index % 10 == 0])\n",
    "\n",
    "plt.savefig(f'images/scatterplot_bleu_reference.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapiro Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "jaccs = result_df[result_df[\"config\"]==\"config_1\"][\"reference_bleu\"].to_numpy()\n",
    "shapiro_test = stats.shapiro(jaccs)\n",
    "print(f\"reference bleu score\",shapiro_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (prefix,config) in non_reference_configs:\n",
    "    df_mask=(result_df['config']==config) & (result_df['prefix']==prefix)\n",
    "    \n",
    "    jaccs1 = result_df[df_mask][\"jaccard_n1\"].to_numpy()\n",
    "    jaccs2 = result_df[df_mask][\"jaccard_n2\"].to_numpy()\n",
    "\n",
    "    shapiro_test1 = stats.shapiro(jaccs1)\n",
    "    shapiro_test2 = stats.shapiro(jaccs2)\n",
    "    print(f\"jacc1_dist\",f\"{prefix}:{config}\",shapiro_test1)\n",
    "    print(f\"jacc2_dist\",f\"{prefix}:{config}\",shapiro_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agg_df_data = []\n",
    "for (prefix,config) in non_reference_configs:\n",
    "    df_mask=(result_df['config']==config) & (result_df['prefix']==prefix)\n",
    "    \n",
    "    bleu_data = result_df[df_mask][\"bleu\"].to_numpy()\n",
    "    jacc_1_data = result_df[df_mask][\"jaccard_n1\"].to_numpy()\n",
    "    jacc_2_data = result_df[df_mask][\"jaccard_n2\"].to_numpy()\n",
    "    \n",
    "    arch = config_archetypes[config]\n",
    "    ts = results[prefix][config][\"properties\"][\"transformations\"]\n",
    "    \n",
    "    shapiro_test = stats.shapiro(bleu_data)\n",
    "    \n",
    "    bleu_median = np.median(bleu_data)\n",
    "    bleu_mean = np.mean(bleu_data)\n",
    "    bleu_iqr = stats.iqr(bleu_data)\n",
    "    \n",
    "    jacc1_median = np.median(jacc_1_data)\n",
    "    jacc1_mean = np.mean(jacc_1_data)\n",
    "    jacc1_iqr = stats.iqr(jacc_1_data)    \n",
    "    \n",
    "    jacc2_median = np.median(jacc_2_data)\n",
    "    jacc2_mean = np.mean(jacc_2_data)\n",
    "    jacc2_iqr = stats.iqr(jacc_2_data)\n",
    "    \n",
    "    config_entry = (prefix,config,arch,ts,\n",
    "                    shapiro_test,\n",
    "                    bleu_median,bleu_mean,bleu_iqr,\n",
    "                    jacc1_median,jacc1_mean,jacc1_iqr,\n",
    "                    jacc2_median,jacc2_mean,jacc2_iqr)\n",
    "    \n",
    "    agg_df_data.append(config_entry)\n",
    "    #print(f\"delta-bleus {config}\",median,mean)\n",
    "    \n",
    "agg_df = pd.DataFrame(agg_df_data) \n",
    "del agg_df_data\n",
    "agg_df.columns=[\n",
    "    \"prefix\",\"config\",\"archetype\",\"transformations\",\n",
    "    \"bleu_shapiro_test\",\n",
    "    \"bleu_median\",\"bleu_mean\",\"bleu_iqr\",\n",
    "    \"jacc1_median\",\"jacc1_mean\",\"jacc1_iqr\",\n",
    "    \"jacc2_median\",\"jacc2_mean\",\"jacc2_iqr\",\n",
    "    \n",
    "]\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    plt.title(f\"Delta-TScore IQR for non-zero delta-tscores in {lan}\")\n",
    "\n",
    "    pivoted_data = agg_df[agg_df[\"prefix\"]==lan].pivot(index='transformations', columns='archetype', values='bleu_iqr')\n",
    "    pivoted_data = pivoted_data.sort_values(\"transformations\",key=lambda col:col.astype(int),ascending=True)\n",
    "    sns.heatmap(pivoted_data, annot=True, fmt=\"g\",cmap='viridis')\n",
    "\n",
    "\n",
    "    plt.savefig(f'images/heatmap_nonzero_shapiro_pvalues_{lan}.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#sns.heatmap(x=\"transformations\",y=\"archetype\",hue=\"delta_tscore_iqr\",center=0,data=filtered_agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    plt.figure(figsize=(21, 7))\n",
    "    plt.grid()\n",
    "\n",
    "    plt.title(f'bleu IQR in {lan}')\n",
    "\n",
    "\n",
    "    sns.barplot(\n",
    "        x=\"config\",y=\"bleu_iqr\",\n",
    "        data=agg_df[agg_df[\"prefix\"]==lan],\n",
    "        hue=\"archetype\",\n",
    "        dodge =False\n",
    "    )\n",
    "\n",
    "    plt.savefig(f'images/barplot_deltatscore_iqrs_{lan}.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non - Setter / Getter Split \n",
    "\n",
    "As we looked in the data, there seems to be a lot of items just for setters and getters that even in the gold standard have a text like \"set the XY\".\n",
    "This is rather noisy, and we want to split the data into \"Setter\",\"Getter\",\"Other\" and have a look at each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_indizes=[]\n",
    "set_indizes=[]\n",
    "low_word_indizes=[]\n",
    "other_indizes=[]\n",
    "\n",
    "def is_in_indizes(index,prefix,lookup):\n",
    "    for (pre,i) in lookup:\n",
    "        if pre == prefix and i == index:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "for lan in languages:\n",
    "    for index in list(results[lan][\"reference\"][\"results\"].keys()):\n",
    "            gold = results[lan][\"reference\"][\"gold_results\"][index]     \n",
    "            words = len(gold.split())\n",
    "\n",
    "            if \"get\" in gold.lower() and words < 10:\n",
    "                get_indizes.append((lan,index)) \n",
    "            elif \"set\" in gold.lower() and not \"setting\" in gold.lower() and words < 10:\n",
    "                set_indizes.append((lan,index))\n",
    "            elif words < 5:\n",
    "                low_word_indizes.append((lan,index))\n",
    "\n",
    "    print(\"gets:\",len(get_indizes))\n",
    "    print(\"sets:\",len(set_indizes))\n",
    "    print(\"low_words:\",len(low_word_indizes))\n",
    "\n",
    "\n",
    "    other_indizes = other_indizes + [ (lan,i) for i in list(results[lan][\"reference\"][\"results\"].keys())\n",
    "                      if not is_in_indizes(i,prefix,get_indizes + set_indizes + low_word_indizes) ]\n",
    "\n",
    "    print(\"remaining indizes:\",len(other_indizes))\n",
    "\n",
    "    # Comment this in for sampling the remaining indizes\n",
    "    #for i in other_indizes[:50]:\n",
    "    #    print(results[\"reference\"][\"gold_results\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_get_bleus = {}\n",
    "ref_getter_bleu = {}\n",
    "ref_set_bleus = {}\n",
    "ref_setter_bleu = {}\n",
    "ref_low_word_bleus = {}\n",
    "ref_lowwords_bleu = {}\n",
    "ref_cleaned_bleus = {}\n",
    "ref_remaining_bleu = {}\n",
    "\n",
    "for lan in languages:\n",
    "    print(lan)\n",
    "    ref_get_bleus[lan] = [results[prefix][\"reference\"][\"bleu_values\"][index] for (prefix,index) in get_indizes if prefix == lan]\n",
    "    ref_getter_bleu[lan] = np.mean(ref_get_bleus[lan])\n",
    "    print(\"get:\",ref_getter_bleu[lan])\n",
    "\n",
    "    ref_set_bleus[lan] = [results[prefix][\"reference\"][\"bleu_values\"][index] for (prefix,index) in set_indizes if prefix == lan]\n",
    "    ref_setter_bleu[lan] = np.mean(ref_set_bleus[lan])\n",
    "    print(\"set:\",ref_setter_bleu[lan])\n",
    "\n",
    "    ref_low_word_bleus[lan] = [results[prefix][\"reference\"][\"bleu_values\"][index] for (prefix,index) in low_word_indizes if prefix == lan]\n",
    "    ref_lowwords_bleu[lan] = np.mean(ref_low_word_bleus[lan])\n",
    "    print(\"low words:\",ref_lowwords_bleu[lan])\n",
    "\n",
    "    ref_cleaned_bleus[lan] = [results[prefix][\"reference\"][\"bleu_values\"][index] for (prefix,index) in other_indizes if prefix == lan]\n",
    "    ref_remaining_bleu[lan] = np.mean(ref_cleaned_bleus[lan])\n",
    "    print(\"remaining indizes:\",ref_remaining_bleu[lan])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_bleus_data = []\n",
    "\n",
    "# For every archetype, add as the 0 transformation point the reference\n",
    "for lan in languages:\n",
    "    for archetype in set(config_archetypes.values()):\n",
    "        datapoint = (lan,\"reference\",archetype,0,\n",
    "                     results[prefix][\"reference\"][\"bleu\"]/100,\n",
    "                     ref_getter_bleu[lan],ref_setter_bleu[lan],ref_lowwords_bleu[lan],ref_remaining_bleu[lan])\n",
    "        split_bleus_data.append(datapoint)\n",
    "\n",
    "# For all configs, make a datapoint with the separated bleus\n",
    "for (lan,config) in non_reference_configs:\n",
    "\n",
    "    archetype = config_archetypes[config]\n",
    "    transformations = results[lan][config][\"properties\"][\"transformations\"]\n",
    "    \n",
    "    getter_bleus = [results[prefix][config][\"bleu_values\"][index] for (prefix,index) in get_indizes if prefix == lan]\n",
    "    getter_agg_bleu = np.mean(getter_bleus)\n",
    "\n",
    "    setter_bleus = [results[prefix][config][\"bleu_values\"][index] for (prefix,index) in set_indizes if prefix == lan]\n",
    "    setter_agg_bleu = np.mean(setter_bleus)\n",
    "\n",
    "    low_word_bleus = [results[prefix][config][\"bleu_values\"][index] for (prefix,index) in low_word_indizes if prefix == lan]\n",
    "    lowwords_agg_bleu = np.mean(low_word_bleus)\n",
    "\n",
    "    other_bleus = [results[prefix][config][\"bleu_values\"][index] for (prefix,index) in other_indizes if prefix == lan]\n",
    "    other_agg_bleu = np.mean(other_bleus)\n",
    "    \n",
    "    datapoint = (lan,config,archetype,transformations,\n",
    "                 results[lan][config][\"bleu\"]/100,\n",
    "                 getter_agg_bleu,setter_agg_bleu,lowwords_agg_bleu,other_agg_bleu)\n",
    "    split_bleus_data.append(datapoint)\n",
    "\n",
    "# Make a dataframe from the values \n",
    "split_bleus_df = pd.DataFrame(split_bleus_data)\n",
    "split_bleus_df.columns = [\n",
    "    \"prefix\",\n",
    "    \"config\",\"archetype\",\"transformations\",\n",
    "    \"bleu\",\n",
    "    \"getter_bleu\",\"setter_bleu\",\"low_word_bleu\",\"remaining_bleu\"\n",
    "]\n",
    "split_bleus_df[\"transformations\"] = split_bleus_df[\"transformations\"].astype(\"int\")\n",
    "split_bleus_df = split_bleus_df.sort_values([\"archetype\",\"transformations\"])\n",
    "split_bleus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_bleus_data_type_b = []\n",
    "\n",
    "# For every archetype, add as the 0 transformation point the reference\n",
    "for lan in languages:\n",
    "    for archetype in set(config_archetypes.values()):\n",
    "        split_bleus_data_type_b.append(\n",
    "            (lan,\"reference\",archetype,0,\"getter_bleu\",ref_getter_bleu[lan])\n",
    "        )\n",
    "        split_bleus_data_type_b.append(\n",
    "            (lan,\"reference\",archetype,0,\"setter_bleu\",ref_setter_bleu[lan])\n",
    "        )\n",
    "        split_bleus_data_type_b.append(\n",
    "            (lan,\"reference\",archetype,0,\"low_word_bleu\",ref_lowwords_bleu[lan])\n",
    "        )\n",
    "        split_bleus_data_type_b.append(\n",
    "            (lan,\"reference\",archetype,0,\"remaining_bleu\",ref_remaining_bleu[lan])\n",
    "        )\n",
    "        split_bleus_data_type_b.append(\n",
    "            (lan,\"reference\",archetype,0,\"bleu\",results[lan][\"reference\"][\"bleu\"]/100)\n",
    "        )\n",
    "\n",
    "# For all configs, make a datapoint with the separated bleus\n",
    "for (prefix,config) in non_reference_configs:\n",
    "\n",
    "    archetype = config_archetypes[config]\n",
    "    transformations = results[prefix][config][\"properties\"][\"transformations\"]\n",
    "    \n",
    "    getter_bleus = [results[prefix][config][\"bleu_values\"][index] for (lan,index) in get_indizes if lan == prefix]\n",
    "    getter_agg_bleu = np.mean(getter_bleus)\n",
    "    split_bleus_data_type_b.append(\n",
    "        (prefix,config,archetype,transformations,\"getter_bleu\",getter_agg_bleu)\n",
    "    )\n",
    "    \n",
    "    setter_bleus = [results[prefix][config][\"bleu_values\"][index] for (lan,index) in set_indizes if lan == prefix]\n",
    "    setter_agg_bleu = np.mean(setter_bleus)\n",
    "    split_bleus_data_type_b.append(\n",
    "        (prefix,config,archetype,transformations,\"setter_bleu\",setter_agg_bleu)\n",
    "    )\n",
    "\n",
    "    low_word_bleus = [results[prefix][config][\"bleu_values\"][index] for (lan,index) in low_word_indizes if lan == prefix]\n",
    "    lowwords_agg_bleu = np.mean(low_word_bleus)\n",
    "    split_bleus_data_type_b.append(\n",
    "        (prefix,config,archetype,transformations,\"low_word_bleu\",lowwords_agg_bleu)\n",
    "    )\n",
    "    other_bleus = [results[prefix][config][\"bleu_values\"][index] for (lan,index) in other_indizes if lan == prefix]\n",
    "    other_agg_bleu = np.mean(other_bleus)\n",
    "    split_bleus_data_type_b.append(\n",
    "        (prefix,config,archetype,transformations,\"remaining_bleu\",other_agg_bleu)\n",
    "    )\n",
    "\n",
    "    split_bleus_data_type_b.append(\n",
    "        (prefix,config,archetype,transformations,\"bleu\",results[prefix][config][\"bleu\"]/100 )\n",
    "    )\n",
    "\n",
    "# Make a dataframe from the values \n",
    "split_bleus_df_type_b = pd.DataFrame(split_bleus_data_type_b)\n",
    "split_bleus_df_type_b.columns = [\n",
    "    \"prefix\",\"config\",\"archetype\",\"transformations\",\"type\",\"value\"\n",
    "]\n",
    "split_bleus_df_type_b[\"transformations\"] = split_bleus_df_type_b[\"transformations\"].astype(\"int\")\n",
    "split_bleus_df_type_b = split_bleus_df_type_b.sort_values([\"archetype\",\"type\",\"transformations\"])\n",
    "split_bleus_df_type_b.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    plt.figure(figsize=(22, 7))\n",
    "    plt.grid()\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=split_bleus_df_type_b[split_bleus_df_type_b[\"prefix\"]==lan],\n",
    "        x=\"transformations\",\n",
    "        y=\"value\",\n",
    "        hue=\"archetype\",\n",
    "        style=\"type\",\n",
    "        marker=True)\n",
    "\n",
    "    plt.xticks([0,1,5,10])\n",
    "    plt.ylabel(f\"Averaged Bleu-Score ({lan})\")\n",
    "\n",
    "    plt.savefig(f'images/bleu_score_per_category_per_archetype_{lan}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    plt.figure(figsize=(22, 7))\n",
    "    plt.grid()\n",
    "\n",
    "    plt.xticks([0,1,5,10])\n",
    "    plt.ylabel(\"Averaged Bleu-Score\")\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=split_bleus_df_type_b[split_bleus_df_type_b[\"prefix\"]==lan],\n",
    "        x=\"transformations\",\n",
    "        y=\"value\",\n",
    "        style=\"type\")\n",
    "    plt.title(f\"Average Bleu Score categorized into getters, setters, low words and others ({lan})\")\n",
    "    plt.xlim(0,10)\n",
    "\n",
    "    plt.savefig(f'images/bleu_score_per_category_{lan}.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count in gold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    data = []\n",
    "    for index in results[lan][\"reference\"][\"gold_results\"].keys():\n",
    "        words = len(results[lan][\"reference\"][\"gold_results\"][index].split())\n",
    "        data.append(words)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.grid()    \n",
    "    sns.histplot(data,bins=50)\n",
    "\n",
    "    plt.title(f\"Distribution of words in gold standard ({lan})\")\n",
    "    plt.xlabel(\"# of words\")\n",
    "    plt.ylabel(\"# of entries\")\n",
    "    plt.xlim(0,100)\n",
    "\n",
    "    plt.xticks(np.arange(0,100,5))\n",
    "\n",
    "    plt.savefig(f'images/word_distribution_goldstandard_{lan}.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    del words,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    data = []\n",
    "    for index in results[lan][\"reference\"][\"results\"].keys():\n",
    "        words = len(results[lan][\"reference\"][\"results\"][index].split())\n",
    "        data.append(words)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.grid()    \n",
    "    sns.histplot(data,bins=50)\n",
    "\n",
    "    plt.title(f\"Distribution of words in reference standard ({lan})\")\n",
    "    plt.xlabel(\"# of words\")\n",
    "    plt.ylabel(\"# of entries\")\n",
    "    plt.xlim(0,100)\n",
    "\n",
    "    plt.xticks(np.arange(0,100,5))\n",
    "\n",
    "    plt.savefig(f'./exports/word_distribution_reference_{lan}.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    del words,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Tests\n",
    "\n",
    "To check whether there are (statistically) significant differences in the groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lan in languages:\n",
    "    ref_bleus = results[lan][\"reference\"][\"bleu_values\"]\n",
    "\n",
    "    wilcoxon_data = []\n",
    "\n",
    "    for (prefix,config) in non_reference_configs:\n",
    "        if prefix == lan:\n",
    "            config_bleus = results[lan][config][\"bleu_values\"]\n",
    "            archetype = config_archetypes[config]\n",
    "            transformations = results[lan][config][\"properties\"][\"transformations\"]\n",
    "\n",
    "            wilcoxon_result = stats.wilcoxon(ref_bleus,config_bleus)\n",
    "            statistic = wilcoxon_result[0]\n",
    "            pvalue = wilcoxon_result[1]\n",
    "\n",
    "            twosided_wilcoxon_result = stats.wilcoxon(ref_bleus,config_bleus,alternative=\"two-sided\")\n",
    "            twosided_statistic = twosided_wilcoxon_result[0]\n",
    "            twosided_pvalue = twosided_wilcoxon_result[1]\n",
    "\n",
    "            datapoint = (lan,config,archetype,transformations,\n",
    "                         statistic,pvalue,\n",
    "                         twosided_statistic,twosided_pvalue)\n",
    "\n",
    "            wilcoxon_data.append(datapoint)\n",
    "\n",
    "    wilcoxon_df = pd.DataFrame(wilcoxon_data)\n",
    "    wilcoxon_df.columns = [\"prefix\",\"config\",\"archetype\",\"transformations\",\n",
    "                           \"wilcoxon_statistics\",\"wilcoxon_pvalue\",\n",
    "                           \"twosided_wilcoxon_statistics\",\"twosided_wilcoxon_pvalue\"]\n",
    "\n",
    "    print(wilcoxon_df.head(7))\n",
    "    wilcoxon_df.to_csv(f'./exports/chi_tests_per_config_{lan}.csv')\n",
    "    \n",
    "    del wilcoxon_df,config_bleus, ref_bleus, wilcoxon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for lan in languages:\n",
    "    lan_ref_bleus = results[lan][\"reference\"][\"bleu_values\"]\n",
    "\n",
    "    friedman_data = []\n",
    "\n",
    "    for (lan2,configA) in non_reference_configs:\n",
    "        if lan == lan2:\n",
    "            configA_bleus = results[lan][configA][\"bleu_values\"]\n",
    "            archetypeA = config_archetypes[configA]\n",
    "            transformationsA = results[lan][configA][\"properties\"][\"transformations\"]\n",
    "\n",
    "            for (lan3,configB) in non_reference_configs:\n",
    "                if lan2 == lan3:\n",
    "                    configB_bleus = results[lan][configB][\"bleu_values\"]\n",
    "                    archetypeB = config_archetypes[configB]\n",
    "                    transformationsB = results[lan][configB][\"properties\"][\"transformations\"]\n",
    "                    friedman_result = stats.friedmanchisquare(lan_ref_bleus,configA_bleus,configB_bleus)\n",
    "                    #print(friedman_result)\n",
    "                    statistic = friedman_result[0]\n",
    "                    pvalue = friedman_result[1]\n",
    "\n",
    "                    datapoint = (\n",
    "                        lan,\n",
    "                        configA,archetypeA,transformationsA,\n",
    "                                 configB,archetypeB,transformationsB,\n",
    "                                 statistic,pvalue)\n",
    "\n",
    "                    friedman_data.append(datapoint)\n",
    "\n",
    "        friedman_df = pd.DataFrame(friedman_data)\n",
    "        friedman_df.columns = [\n",
    "            \"prefix\",\n",
    "            \"configA\",\"archetypeA\",\"transformationsA\",\n",
    "            \"configB\",\"archetypeB\",\"transformationsB\",\n",
    "            \"friedman_statistics\",\"friedman_pvalue\"]\n",
    "\n",
    "\n",
    "    print(friedman_df.head())\n",
    "        \n",
    "    del configB_bleus,configA_bleus, lan_ref_bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.title(\"Friedman-PValue \\nof ConfigA<>ConfigB<>Reference\")\n",
    "\n",
    "ffriedman_df = friedman_df.copy()\n",
    "ffriedman_df['configA'] = friedman_df['configA'].apply(config_num)\n",
    "ffriedman_df['configA'].astype(int)\n",
    "ffriedman_df['configB'] = friedman_df['configB'].apply(config_num)\n",
    "ffriedman_df['configB'].astype(int)\n",
    "\n",
    "pivoted_data = ffriedman_df.pivot(index='configA', columns='configB', values='friedman_pvalue')\n",
    "sns.heatmap(pivoted_data, annot=False, fmt=\"g\",cmap='viridis')\n",
    "\n",
    "\n",
    "plt.savefig(f'./exports/friedman_pvalues_bleuscore.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = result_df[(result_df[\"difference\"])]\n",
    "plot_df = plot_df[(plot_df[\"transformations\"]=='1')]\n",
    "\n",
    "plot_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export \n",
    "\n",
    "This can be used to print a pdf (or html). Comment it in if you want to do so. \n",
    "\n",
    "--to=pdf takes quite a while, --to=html is pretty fast. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# export to csv for R usage\n",
    "method_dict = {}\n",
    "for lan in languages:\n",
    "    method_dict[lan] = {}\n",
    "    for (p,i) in get_indizes:\n",
    "        if p == lan:\n",
    "            method_dict[lan][i]=\"Getter\"\n",
    "    for (p,i) in set_indizes:\n",
    "        if p == lan:\n",
    "            method_dict[lan][i]=\"Setter\"\n",
    "    for (p,i) in low_word_indizes:\n",
    "        if p == lan:\n",
    "            method_dict[lan][i]=\"Low_Words\"\n",
    "    for (p,i) in other_indizes:\n",
    "        if p == lan:\n",
    "            method_dict[lan][i]=\"Normal\"\n",
    "\n",
    "csv_export_data  = []\n",
    "\n",
    "for lan in languages:\n",
    "    for index in results[lan][\"reference\"][\"results\"].keys():\n",
    "        ref_data = results[lan][\"reference\"][\"results\"][index]\n",
    "        gold_data = results[lan][\"reference\"][\"gold_results\"][index]\n",
    "        ref_bleu =  results[lan][\"reference\"][\"bleu_values\"][index]\n",
    "        ref_length = len(ref_data)\n",
    "        ref_word_length = len(ref_data.split())\n",
    "        ref_jacc1_to_ref = 0 \n",
    "\n",
    "        ref_jacc_1_to_gold = jaccard_wrapper(ref_data,gold_data)\n",
    "        ref_perfect = gold_data == ref_data\n",
    "\n",
    "        method_type = method_dict[lan][index]\n",
    "\n",
    "        ref_datapoint = (lan,\n",
    "            \"reference\",\"none\",\"none\",\"0\", method_type,index,\n",
    "            #ref_data,\n",
    "            ref_bleu,ref_jacc1_to_ref,ref_jacc_1_to_gold,ref_length, \n",
    "            False,ref_perfect \n",
    "        )\n",
    "        csv_export_data.append(ref_datapoint)\n",
    "\n",
    "        for (prefix,config) in non_reference_configs:\n",
    "            if prefix == lan:\n",
    "                conf_data = results[prefix][config][\"results\"][index]\n",
    "                #print(config,conf_data,ref_data)\n",
    "                arch = config_archetypes[config]\n",
    "                mt = archetype_mt_mapping[arch]\n",
    "                ts = results[prefix][config][\"properties\"][\"transformations\"]\n",
    "\n",
    "                bleu =  results[prefix][config][\"bleu_values\"][index]\n",
    "                length = len(conf_data)\n",
    "                word_length = len(conf_data.split())\n",
    "\n",
    "                diff = conf_data != ref_data\n",
    "                perfect = conf_data == gold_data\n",
    "\n",
    "                result_df_line = result_df[(result_df[\"prefix\"]==prefix) & (result_df[\"config\"]==config)  &  (result_df[\"index\"]==index)]\n",
    "\n",
    "                jacc_1_to_ref = result_df_line[\"jaccard_n1_reference\"].iloc(0)[0]\n",
    "                jacc_1_to_gold = result_df_line[\"jaccard_n1\"].iloc(0)[0]\n",
    "\n",
    "                conf_datapoint = (\n",
    "                    lan,\n",
    "                    config,arch,mt,ts, method_type,index,\n",
    "                    #ref_data,\n",
    "                    bleu,jacc_1_to_ref,jacc_1_to_gold,\n",
    "                    length,word_length,\n",
    "                    diff, perfect\n",
    "                )\n",
    "                csv_export_data.append(conf_datapoint)\n",
    "    #print(index)\n",
    "csv_export_df = pd.DataFrame(csv_export_data)\n",
    "csv_export_df.columns = [\"prefix\",\n",
    "    \"config\",\"archetype\",\"MT\",\"transformations\",\"method_type\",\"entry\",\n",
    "    \"bleu_score\",\n",
    "    \"jaccard_distance_to_gold\",\"jaccard_distance_to_reference\",\n",
    "    \"length_in_characters\", \"length_in_words\",\n",
    "    \"different_to_ref\", \"perfect_match_with_gold\"\n",
    "]\n",
    "csv_export_df.to_csv(\"./exports/bleus.csv\")\n",
    "csv_export_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del csv_export_df\n",
    "!jupyter nbconvert --to=pdf --output-dir=./exports Evaluation.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
