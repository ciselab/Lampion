{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CodeBert Grid Experiment Evaluation\n",
    "\n",
    "Nice to see you around! Have a seat.\n",
    "Would you like a drink? Maybe a cigar?\n",
    "\n",
    "Make sure to have all required dependencies installed - they are listed in the [environment.yml](./environment.yml). \n",
    "You create a conda environment from the yml using \n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate Lampion-Codebert-Evaluation\n",
    "```\n",
    "\n",
    "Make sure to run your Jupyter Notebook from that environment! \n",
    "Otherwise you are (still) missing the dependencies. \n",
    "\n",
    "**OPTIONALLY** you can use the environment in which your jupter notebook is already running, with starting a new terminal (from jupyter) and run \n",
    "\n",
    "```\n",
    "conda env update --prefix ./env --file environment.yml  --prune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Homebrew Imports (python-file next to this)\n",
    "import bleu_evaluator as foreign_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Loading / Preparation\n",
    "\n",
    "Make sure that your dataset looks like described in the [Readme](./README.md), that is \n",
    "\n",
    "```\n",
    "./data\n",
    "    /GridExp_XY\n",
    "        reference.gold\n",
    "        reference.output\n",
    "        /config_0\n",
    "            config.properties\n",
    "            test_0.output\n",
    "        /config_1\n",
    "            config.properties\n",
    "            test_0.output\n",
    "    ...\n",
    "```\n",
    "\n",
    "where the configs **must** be numbered to be correctly detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_directory = \"./data/PreliminaryResults\"\n",
    "\n",
    "print(f\"looking for results in {data_directory}\" )\n",
    "\n",
    "property_files={}\n",
    "result_files={}\n",
    "\n",
    "for root,dirs,files in os.walk(data_directory):\n",
    "    for name in files:\n",
    "        if \"reference.gold\" in name:\n",
    "            result_files[\"gold\"]=os.path.join(root,name)\n",
    "        elif \"reference.output\" in name:\n",
    "            result_files[\"reference\"]=os.path.join(root,name)\n",
    "        elif \"config_\" in root and \".output\" in name:\n",
    "            result_files[os.path.basename(root)]=os.path.join(root,name)\n",
    "        elif \"config_\" in root and \".properties\" in name:\n",
    "            property_files[os.path.basename(root)]=os.path.join(root,name)\n",
    "\n",
    "print(f\"There were {len(result_files)} result- and {len(property_files)} property-files found.\")\n",
    "# Sanity Checks \n",
    "if \"gold\" not in result_files.keys():\n",
    "    print(\"There was no Gold-file for the results found!\")\n",
    "if \"reference\" not in result_files.keys():\n",
    "    print(\"There was no reference-file for the results found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bare results\n",
    "results = {}\n",
    "# Fill them with file-paths\n",
    "for (key,value) in result_files.items():\n",
    "    results[key] = {}\n",
    "    results[key][\"result_file\"] = value\n",
    "\n",
    "for (key,value) in property_files.items():\n",
    "    results[key][\"property_file\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    \"\"\"\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    \"\"\"\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in property-files\")\n",
    "\n",
    "for key in property_files.keys():\n",
    "    prop_file = results[key][\"property_file\"]\n",
    "    if prop_file:\n",
    "        results[f\"{key}\"][\"properties\"]=load_properties(prop_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in result-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    result_file = results[key][\"result_file\"]\n",
    "    if result_file:\n",
    "        f = open(result_file)\n",
    "        lines=f.readlines()\n",
    "        results[key][\"results\"]={}\n",
    "        for l in lines:\n",
    "            num = int(l.split(\"\\t\")[0])\n",
    "            content = l.split(\"\\t\")[1]\n",
    "            content = content.strip()\n",
    "            results[key][\"results\"][num] = content\n",
    "        f.close()\n",
    "\n",
    "# Comment this in for inspection of results\n",
    "#results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu-Scores\n",
    "\n",
    "In the following, the BLEU-scores will be calculated using the foreign libary. \n",
    "While there have been minor changes to standard-BLEU, it is the same as used in the original experiment.\n",
    "\n",
    "The aggregated BLEU-Scores will be stored to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BLEU-Score of the un-altered Set:\")\n",
    "ref_bleu = foreign_bleu.bleuFromMaps(results[\"gold\"][\"results\"],results[\"reference\"][\"results\"])\n",
    "print(ref_bleu[0])\n",
    "print(\"BLEU-Score of the if-true config:\")\n",
    "c0_bleu = foreign_bleu.bleuFromMaps(results[\"gold\"][\"results\"],results[\"config_0\"][\"results\"])\n",
    "print(c0_bleu[0])\n",
    "print(\"BLEU-Score of the gold set:\")\n",
    "gold_bleu = foreign_bleu.bleuFromMaps(results[\"gold\"][\"results\"],results[\"gold\"][\"results\"])\n",
    "print(gold_bleu[0],\"(Should be 100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"calculating bleu scores for all configs\")\n",
    "for key in results.keys():\n",
    "    if \"gold\" not in key:\n",
    "        bleu = foreign_bleu.bleuFromMaps(results[\"gold\"][\"results\"],results[key][\"results\"])\n",
    "        results[key][\"bleu\"]=bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = {0:\"if-true\", 1:\"add-var(pseudo)\", 2:\"add-neutral\",3:\"add-var(random)\"}\n",
    "i = 0\n",
    "bleu_plots = {}\n",
    "while i < len(property_files.keys()):\n",
    "    config_type = config_names[i // 3]\n",
    "    if config_type in bleu_plots.keys():\n",
    "        c = results[f\"config_{i}\"]\n",
    "        bleu_plots[config_type][int(c[\"properties\"][\"transformations\"])] = c[\"bleu\"][0]\n",
    "    else:\n",
    "        bleu_plots[config_type]={}\n",
    "        bleu_plots[config_type][0]=results[\"reference\"][\"bleu\"][0]\n",
    "        c = results[f\"config_{i}\"]\n",
    "        bleu_plots[config_type][int(c[\"properties\"][\"transformations\"])] = c[\"bleu\"][0]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubi = pd.DataFrame.from_dict(bleu_plots)\n",
    "ubi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"BLEU-Score\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(ubi,marker=\"o\")\n",
    "\n",
    "plt.legend(ubi.columns)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
