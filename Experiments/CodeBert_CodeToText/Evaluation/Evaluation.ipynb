{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CodeBert Grid Experiment Evaluation\n",
    "\n",
    "Nice to see you around! Have a seat.\n",
    "Would you like a drink? Maybe a cigar?\n",
    "\n",
    "Make sure to have all required dependencies installed - they are listed in the [environment.yml](./environment.yml). \n",
    "You create a conda environment from the yml using \n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate Lampion-Codebert-Evaluation\n",
    "```\n",
    "\n",
    "Make sure to run your Jupyter Notebook from that environment! \n",
    "Otherwise you are (still) missing the dependencies. \n",
    "\n",
    "**OPTIONALLY** you can use the environment in which your jupter notebook is already running, with starting a new terminal (from jupyter) and run \n",
    "\n",
    "```\n",
    "conda env update --prefix ./env --file environment.yml  --prune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "# Homebrew Imports (python-file next to this)\n",
    "import bleu_evaluator as foreign_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data-Loading / Preparation\n",
    "\n",
    "Make sure that your dataset looks like described in the [Readme](./README.md), that is \n",
    "\n",
    "```\n",
    "./data\n",
    "    /GridExp_XY\n",
    "        /configs\n",
    "            /reference\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_0\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_1\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "    ...\n",
    "```\n",
    "\n",
    "where the configs **must** be numbered to be correctly detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs the bleu-score upon the config files, creating the bleu.txt's \n",
    "# If your data package was provided including the txt you dont need to do this. \n",
    "# Existing bleu.txt's will be overwritten. \n",
    "\n",
    "#!./metric_runner.sh ./data/PreliminaryResults/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_directory = \"./data/PreliminaryResults\"\n",
    "\n",
    "# These archetypes are later used to group the configurations\n",
    "# While to grouping is up to you, right here it simply is one archetype for each transformation type, \n",
    "# Grouping together different configs with the same transformations applied (but different #Transformations)\n",
    "config_archetypes = {\n",
    "    \"config_0\":\"if-true\",\"config_1\":\"if-true\",\"config_2\":\"if-true\",\n",
    "    \"config_3\":\"mixed names(pseudo)\",\"config_4\":\"mixed names(pseudo)\",\"config_5\":\"mixed names(pseudo)\",\n",
    "    \"config_6\":\"add-neutral\",\"config_7\":\"add-neutral\",\"config_8\":\"add-neutral\",\n",
    "    \"config_9\":\"mixed-names(random)\",\"config_10\":\"mixed-names(random)\",\"config_11\":\"mixed-names(random)\"\n",
    "}\n",
    "\n",
    "print(f\"looking for results in {data_directory}\" )\n",
    "\n",
    "results={}\n",
    "\n",
    "for root,dirs,files in os.walk(data_directory):\n",
    "    for name in files:\n",
    "        if \".gold\" in name:\n",
    "            directory = os.path.basename(root)\n",
    "            results[directory]={}\n",
    "            \n",
    "            results[directory][\"result_file\"]=os.path.join(root,\"test_0.output\")\n",
    "            results[directory][\"gold_file\"]=os.path.join(root,\"test_0.gold\")\n",
    "            results[directory][\"bleu_file\"]=os.path.join(root,\"bleu.txt\")\n",
    "            if os.path.exists(os.path.join(root,\"config.properties\")):\n",
    "                results[directory][\"property_file\"]=os.path.join(root,\"config.properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    \"\"\"\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    \"\"\"\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "print(\"reading in property-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    if \"property_file\" in results[key].keys():\n",
    "        results[f\"{key}\"][\"properties\"]=load_properties(results[key][\"property_file\"])\n",
    "\n",
    "print(\"done reading the properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in result-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    result_file = results[key][\"result_file\"]\n",
    "    f = open(result_file)\n",
    "    lines=f.readlines()\n",
    "    results[key][\"results\"]={}\n",
    "    for l in lines:\n",
    "        num = int(l.split(\"\\t\")[0])\n",
    "        content = l.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"results\"][num] = content\n",
    "    f.close()\n",
    "    \n",
    "    gold_file = results[key]['gold_file']\n",
    "    gf = open(gold_file)\n",
    "    glines=gf.readlines()\n",
    "    results[key][\"gold_results\"]={}\n",
    "    for gl in glines:\n",
    "        num = int(gl.split(\"\\t\")[0])\n",
    "        content = gl.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"gold_results\"][num] = content\n",
    "    gf.close()\n",
    "\n",
    "print(\"done reading the result files\")\n",
    "# Comment this in for inspection of results\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in the bleu-scores\")\n",
    "\n",
    "for key in results.keys():\n",
    "    bleu_file = results[key][\"bleu_file\"]\n",
    "    f = open(bleu_file)\n",
    "    score=f.readlines()[0]\n",
    "    results[key][\"bleu\"]=float(score)\n",
    "    f.close()\n",
    "    \n",
    "print(\"done reading the bleu-scores\")\n",
    "\n",
    "#results[\"config_0\"][\"bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in results.keys():\n",
    "    if \"property_file\" in results[key].keys():\n",
    "        results[key][\"archetype\"]=config_archetypes[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bleu-Scores\n",
    "\n",
    "In the following, the BLEU-scores will be calculated using the foreign libary. \n",
    "While there have been minor changes to standard-BLEU, it is the same as used in the original experiment.\n",
    "\n",
    "The aggregated BLEU-Scores will be stored to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    bleu_data[archetype]={}\n",
    "    bleu_data[archetype][0]=results[\"reference\"][\"bleu\"]\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        bleu_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"bleu\"]\n",
    "   \n",
    "bleu_data_df = pd.DataFrame.from_dict(bleu_data)\n",
    "bleu_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"BLEU-Score\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(bleu_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(bleu_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "Before the samples can be inspected, the items need to be re-indexed. \n",
    "While all config_results are in the reference_results, there might is an issue with the data being shuffeld. \n",
    "\n",
    "To fix this, a reindexing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Reindexing Pseudocode\n",
    "\n",
    "def lookup_index(sentence, comparison_dict):\n",
    "    for (gold_key,gold_value) in comparison_dict.items():\n",
    "        if sentence == gold_value:\n",
    "            return gold_key\n",
    "    return -1\n",
    "\n",
    "# For each config (that is not reference)\n",
    "    # Create a lookup of reference_gold_index -> config_gold_index\n",
    "    # Invert the lookup \n",
    "    # Make a new dictionary where\n",
    "        # For every key of the config_gold\n",
    "        # lookup the key of the reference_gold\n",
    "        # And fill it with {reference_gold_key : config_gold_value}\n",
    "        # Do the same with the non-gold results\n",
    "        # Fill it with {reference_gold_key : config_result_value}\n",
    "    # Set result[config_X][\"gold_results\"] to the newly created, matching index one \n",
    "    # same for non-gold-results\n",
    "    \n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    keyMapping={}\n",
    "    for (k,v) in results[config][\"gold_results\"].items():\n",
    "        gk = lookup_index(v,results[\"reference\"][\"gold_results\"])\n",
    "        keyMapping[k]=gk\n",
    "    new_gold_results={}\n",
    "    new_results={}\n",
    "    for (config_key,gold_key) in keyMapping.items():\n",
    "        if gold_key != -1:\n",
    "            new_gold_results[gold_key]=results[config][\"gold_results\"][config_key]\n",
    "            new_results[gold_key]=results[config][\"results\"][config_key]\n",
    "    results[config][\"gold_results\"]=new_gold_results\n",
    "    results[config][\"results\"]=new_results\n",
    "    #print(config,keyMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 250\n",
    "print(results[\"reference\"][\"gold_results\"][sample_index] )\n",
    "#print(results[\"config_2\"][\"gold_results\"][sample_index])\n",
    "print()\n",
    "print(results[\"reference\"][\"results\"][sample_index])\n",
    "print(results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_wrapper(sentenceA,sentenceB,ngram=1):\n",
    "    tokensA = nltk.word_tokenize(sentenceA)\n",
    "    tokensB = nltk.word_tokenize(sentenceB)\n",
    "\n",
    "    ngA_tokens = set(nltk.ngrams(tokensA, n=ngram))\n",
    "    ngB_tokens = set(nltk.ngrams(tokensB, n=ngram))\n",
    "    \n",
    "    return nltk.jaccard_distance(ngA_tokens, ngB_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_wrapper(sentence_to_check,reference):\n",
    "    check_tokens = nltk.word_tokenize(sentence_to_check)\n",
    "    ref_tokens = nltk.word_tokenize(reference)\n",
    "    \n",
    "    # From comparing the foreign_bleu and nltk the method4 seems to match\n",
    "    # The Paper mentiones the BLEU-4-Score with a citation to chen & cherry\n",
    "    # I wish I could be named chen & cherry, its a very cool name. \n",
    "    chencherry = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    smooth_fn = chencherry.method4\n",
    "    \n",
    "    return nltk.translate.bleu_score.sentence_bleu([ref_tokens],check_tokens,smoothing_function=smooth_fn)\n",
    "\n",
    "bleu_wrapper(results[\"config_2\"][\"results\"][sample_index],results[\"config_2\"][\"gold_results\"][sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Scores\n",
    "\n",
    "from the paper, we provide two ways of checking on the transformation score: \n",
    "\n",
    "```\n",
    "delta_tscore = bleu(gold,reference) - bleu(gold,altered)\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```\n",
    "tquot = bleu(gold,altered) / bleu(gold,reference)\n",
    "```\n",
    "\n",
    "which we can perfectly write as python and see how they are doing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_tscore = lambda gold,reference,altered : bleu_wrapper(reference,gold) - bleu_wrapper(altered,gold)\n",
    "\n",
    "tquot = lambda gold,reference,altered : bleu_wrapper(altered,gold) / bleu_wrapper(reference,gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_tscore(results[\"config_2\"][\"gold_results\"][sample_index],results[\"reference\"][\"results\"][sample_index],results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tquot(results[\"config_2\"][\"gold_results\"][sample_index],results[\"reference\"][\"results\"][sample_index],results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hall of Shame** \n",
    "\n",
    "worst entries in terms of t-score (either one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 10\n",
    "worst_n = []\n",
    "best_n = []\n",
    "\n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        try:\n",
    "            r = (delta_tscore(gold,reference,altered),config,index)\n",
    "        except ZeroDivisionError: \n",
    "            print(\"Error at \",index,config)\n",
    "        else:\n",
    "            running_agg.append(r)\n",
    "            \n",
    "    running_agg = sorted(running_agg + worst_n + best_n,key=lambda x:x[0])\n",
    "    worst_n=running_agg[:n]\n",
    "    best_n=running_agg[-n:]\n",
    "    del running_agg\n",
    "    #print(\"ckecked for worst in \",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (diff,config,index) in worst_n+best_n:\n",
    "    print(\n",
    "        f\"delta_tscore {round(diff,4)}, {config}@{index}\\n\",\n",
    "        f'gold: \\t {results[\"reference\"][\"gold_results\"][index]} \\n',\n",
    "        f'ref: \\t {results[\"reference\"][\"results\"][index]}\\n',\n",
    "        f'alt: \\t {results[config][\"results\"][index]}\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m = 10\n",
    "worst_m = []\n",
    "best_m = []\n",
    "\n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = (tquot(gold,reference,altered),config,index)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            #there are quite a lot of errors \n",
    "            r = (0,config,index)\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    running_agg = sorted(running_agg + worst_m + best_m,key=lambda x:x[0])\n",
    "    worst_m=running_agg[:m]\n",
    "    best_m=running_agg[-m:]\n",
    "    del running_agg\n",
    "    #print(\"ckecked for worst in \",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (diff,config,index) in worst_n+best_n:\n",
    "    print(\n",
    "        f\"tquot {round(diff,4)}, {config}@{index}\\n\",\n",
    "        f'gold: \\t {results[\"reference\"][\"gold_results\"][index]} \\n',\n",
    "        f'ref: \\t {results[\"reference\"][\"results\"][index]}\\n',\n",
    "        f'alt: \\t {results[config][\"results\"][index]}\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Histogram of TScores & TQuots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
