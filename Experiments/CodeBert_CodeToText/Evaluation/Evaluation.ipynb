{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CodeBert Grid Experiment Evaluation\n",
    "\n",
    "Nice to see you around! Have a seat.\n",
    "Would you like a drink? Maybe a cigar?\n",
    "\n",
    "Make sure to have all required dependencies installed - they are listed in the [environment.yml](./environment.yml). \n",
    "You create a conda environment from the yml using \n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate Lampion-Codebert-Evaluation\n",
    "```\n",
    "\n",
    "Make sure to run your Jupyter Notebook from that environment! \n",
    "Otherwise you are (still) missing the dependencies. \n",
    "\n",
    "**OPTIONALLY** you can use the environment in which your jupter notebook is already running, with starting a new terminal (from jupyter) and run \n",
    "\n",
    "```\n",
    "conda env update --prefix ./env --file environment.yml  --prune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "# Homebrew Imports (python-file next to this)\n",
    "import bleu_evaluator as foreign_bleu\n",
    "\n",
    "# Set Jupyter vars\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data-Loading / Preparation\n",
    "\n",
    "Make sure that your dataset looks like described in the [Readme](./README.md), that is \n",
    "\n",
    "```\n",
    "./data\n",
    "    /GridExp_XY\n",
    "        /configs\n",
    "            /reference\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_0\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_1\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "    ...\n",
    "```\n",
    "\n",
    "where the configs **must** be numbered to be correctly detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs the bleu-score upon the config files, creating the bleu.txt's \n",
    "# If your data package was provided including the txt you dont need to do this. \n",
    "# Existing bleu.txt's will be overwritten. \n",
    "\n",
    "#!./metric_runner.sh ./data/PreliminaryResults/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_directory = \"./data/PreliminaryResults\"\n",
    "\n",
    "# These archetypes are later used to group the configurations\n",
    "# While to grouping is up to you, right here it simply is one archetype for each transformation type, \n",
    "# Grouping together different configs with the same transformations applied (but different #Transformations)\n",
    "config_archetypes = {\n",
    "    \"config_0\":\"if-true\",\"config_1\":\"if-true\",\"config_2\":\"if-true\",\n",
    "    \"config_3\":\"mixed names(pseudo)\",\"config_4\":\"mixed names(pseudo)\",\"config_5\":\"mixed names(pseudo)\",\n",
    "    \"config_6\":\"add-neutral\",\"config_7\":\"add-neutral\",\"config_8\":\"add-neutral\",\n",
    "    \"config_9\":\"mixed-names(random)\",\"config_10\":\"mixed-names(random)\",\"config_11\":\"mixed-names(random)\",\n",
    "    \"config_12\": \"add-var(pseudo)\",\"config_13\": \"add-var(pseudo)\",\"config_14\": \"add-var(pseudo)\",\n",
    "    \"config_15\": \"add-var(random)\",\"config_16\": \"add-var(random)\",\"config_17\": \"add-var(random)\",\n",
    "    \"config_18\": \"if-true & add-neutral\",\"config_19\": \"if-true & add-neutral\",\"config_20\": \"if-true & add-neutral\"\n",
    "}\n",
    "\n",
    "print(f\"looking for results in {data_directory}\" )\n",
    "\n",
    "results={}\n",
    "\n",
    "for root,dirs,files in os.walk(data_directory):\n",
    "    for name in files:\n",
    "        if \".gold\" in name:\n",
    "            directory = os.path.basename(root)\n",
    "            results[directory]={}\n",
    "            \n",
    "            results[directory][\"result_file\"]=os.path.join(root,\"test_0.output\")\n",
    "            results[directory][\"gold_file\"]=os.path.join(root,\"test_0.gold\")\n",
    "            results[directory][\"bleu_file\"]=os.path.join(root,\"bleu.txt\")\n",
    "            if os.path.exists(os.path.join(root,\"config.properties\")):\n",
    "                results[directory][\"property_file\"]=os.path.join(root,\"config.properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    \"\"\"\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    \"\"\"\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "print(\"reading in property-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    if \"property_file\" in results[key].keys():\n",
    "        results[f\"{key}\"][\"properties\"]=load_properties(results[key][\"property_file\"])\n",
    "\n",
    "print(\"done reading the properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in result-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    result_file = results[key][\"result_file\"]\n",
    "    f = open(result_file)\n",
    "    lines=f.readlines()\n",
    "    results[key][\"results\"]={}\n",
    "    for l in lines:\n",
    "        num = int(l.split(\"\\t\")[0])\n",
    "        content = l.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"results\"][num] = content\n",
    "    f.close()\n",
    "    \n",
    "    gold_file = results[key]['gold_file']\n",
    "    gf = open(gold_file)\n",
    "    glines=gf.readlines()\n",
    "    results[key][\"gold_results\"]={}\n",
    "    for gl in glines:\n",
    "        num = int(gl.split(\"\\t\")[0])\n",
    "        content = gl.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"gold_results\"][num] = content\n",
    "    gf.close()\n",
    "\n",
    "print(\"done reading the result files\")\n",
    "# Comment this in for inspection of results\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in the bleu-scores\")\n",
    "\n",
    "for key in results.keys():\n",
    "    bleu_file = results[key][\"bleu_file\"]\n",
    "    f = open(bleu_file)\n",
    "    score=f.readlines()[0]\n",
    "    results[key][\"bleu\"]=float(score)\n",
    "    f.close()\n",
    "    \n",
    "print(\"done reading the bleu-scores\")\n",
    "\n",
    "#results[\"config_0\"][\"bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in results.keys():\n",
    "    if \"property_file\" in results[key].keys():\n",
    "        results[key][\"archetype\"]=config_archetypes[key]\n",
    "        \n",
    "non_reference_configs = [key for key in results.keys() if \"reference\" != key]\n",
    "\n",
    "def archetype_info(config):\n",
    "    archetype = config_archetypes[config]\n",
    "    transforms = int(results[config][\"properties\"][\"transformations\"])\n",
    "    return (archetype,transforms)\n",
    "\n",
    "print_archetype_info = lambda config: f\"{(archetype_info(config))[0]}@{(archetype_info(config))[1]}\"\n",
    "\n",
    "all_archetypes = set(config_archetypes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_wrapper(sentenceA,sentenceB,ngram=1):\n",
    "    tokensA = nltk.word_tokenize(sentenceA)\n",
    "    tokensB = nltk.word_tokenize(sentenceB)\n",
    "\n",
    "    ngA_tokens = set(nltk.ngrams(tokensA, n=ngram))\n",
    "    ngB_tokens = set(nltk.ngrams(tokensB, n=ngram))\n",
    "    \n",
    "    return nltk.jaccard_distance(ngA_tokens, ngB_tokens)\n",
    "\n",
    "def bleu_wrapper(sentence_to_check,reference):\n",
    "    check_tokens = nltk.word_tokenize(sentence_to_check)\n",
    "    ref_tokens = nltk.word_tokenize(reference)\n",
    "    \n",
    "    # From comparing the foreign_bleu and nltk the method4 seems to match\n",
    "    # The Paper mentiones the BLEU-4-Score with a citation to chen & cherry\n",
    "    # I wish I could be named chen & cherry, its a very cool name. \n",
    "    chencherry = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    smooth_fn = chencherry.method4\n",
    "    \n",
    "    try:\n",
    "        return nltk.translate.bleu_score.sentence_bleu([ref_tokens],check_tokens,smoothing_function=smooth_fn)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "sample_index=125\n",
    "bleu_wrapper(results[\"config_2\"][\"results\"][sample_index],results[\"config_2\"][\"gold_results\"][sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bleu-Scores\n",
    "\n",
    "In the following, the BLEU-scores will be calculated using the foreign libary. \n",
    "While there have been minor changes to standard-BLEU, it is the same as used in the original experiment.\n",
    "\n",
    "The aggregated BLEU-Scores will be stored to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bleu_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    bleu_data[archetype]={}\n",
    "    bleu_data[archetype][0]=results[\"reference\"][\"bleu\"]\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        bleu_data[archetype][int(results[c][\"properties\"][\"transformations\"])]=results[c][\"bleu\"]\n",
    "   \n",
    "bleu_data_df = pd.DataFrame.from_dict(bleu_data)\n",
    "#bleu_data_df = bleu_data_df.sort_index() #(by=\"index\")\n",
    "bleu_data_df = bleu_data_df.sort_index()\n",
    "bleu_data_df = bleu_data_df.applymap(lambda cell: round(cell,3))\n",
    "\n",
    "\n",
    "with open(\"./exports/bleu_table.tex\",\"w\") as f:\n",
    "    # I maybe want to consider column_format=\"{rrrlll}\" etc. \n",
    "    f.write(\n",
    "        bleu_data_df.to_latex(\n",
    "            caption=\"BLEU4-Scores for increasing number of metamorphic transformations \\n (applied n-times per datapoint)\"\n",
    "            ,label=\"tab:bleu_scores\"\n",
    "            ,position=\"h\"\n",
    "        )         \n",
    "    )\n",
    "\n",
    "bleu_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.ylabel(\"BLEU-Score\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "#for latex, its nicer to have the title set from latex itself\n",
    "#plt.title(\"BLEU4-Scores for increasing number of metamorphic transformations \\n (applied n-times per datapoint)\")\n",
    "\n",
    "plot = sns.lineplot(data=bleu_data_df,markers=True,style=None,dashes=False)\n",
    "plt.xticks([0,1,5,10])\n",
    "plt.xlim(-0.025,10.1)\n",
    "plt.legend(bleu_data_df.columns)\n",
    "plt.savefig('images/bleu_scores.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleus = lambda config_id : [\n",
    "    bleu_wrapper(results[config_id][\"results\"][i],results[config_id][\"gold_results\"][i]) \n",
    "    for i \n",
    "    in results[config_id][\"results\"].keys()\n",
    "]\n",
    "\n",
    "\n",
    "bleus_reference_data = calculate_bleus(\"reference\")\n",
    "sample_bleus_config_data = calculate_bleus(\"config_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bleu_histogram(config_data,reference_data,title):\n",
    "    plt.figure(figsize=(14,7))\n",
    "    \n",
    "    histo_df=pd.DataFrame.from_dict(\n",
    "        {\"reference\":reference_data,\n",
    "            title:config_data }\n",
    "    )\n",
    "\n",
    "    sns.displot(\n",
    "        data=histo_df,\n",
    "        kind=\"hist\", kde=True,\n",
    "        height=6, aspect=10/6\n",
    "               )\n",
    "    plt.title(f\"Histogram of Bleu-Scores for {title}\")\n",
    "    plt.xlabel(\"Bleu-Score\")\n",
    "    #plt.ylabel(\"# of Entries\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.savefig(f'images/{title}_bleu_histogram.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_bleu_boxplot(config_data,reference_data,title=None):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    box_df=pd.DataFrame.from_dict(\n",
    "        {\"reference\":reference_data,\n",
    "            title:config_data }\n",
    "    )    \n",
    "    sns.boxplot(\n",
    "        data=box_df)\n",
    "    \n",
    "    plt.title(f\"Boxplot of Bleu-Scores for {title}\")\n",
    "    plt.ylabel(\"Bleu-Score\")\n",
    "    \n",
    "    major_ticks = np.arange(0, 1, 0.2)\n",
    "    minor_ticks = np.arange(0, 1, 0.05)\n",
    "\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "\n",
    "    # And a corresponding grid\n",
    "    ax.grid(which='both')\n",
    "\n",
    "    #plt.grid()\n",
    "    plt.savefig(f'images/{title}_bleu_box.png')\n",
    "    plt.ylim(0,1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_bleu_violinplot(config_data,reference_data,title):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    violin_df=pd.DataFrame.from_dict(\n",
    "        {\"reference\":reference_data,\n",
    "            title:config_data }\n",
    "    )\n",
    "    \n",
    "    sns.violinplot(data=violin_df)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.title(f\"ViolinPlot of Bleu-Scores for {title}\")\n",
    "    plt.ylabel(\"Bleu-Score\")\n",
    "    \n",
    "    plt.savefig(f'images/{title}_bleu_violin.png')\n",
    "    plt.show()\n",
    "    \n",
    "#plot_bleu_violinplot(sample_bleus_config_data,bleus_reference_data,\"config_20\")\n",
    "plot_bleu_boxplot(sample_bleus_config_data,bleus_reference_data,\"config_20\")\n",
    "#plot_bleu_histogram(sample_bleus_config_data,bleus_reference_data,\"config_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bleus_reference_data = calculate_bleus(\"reference\")\n",
    "results[\"reference\"][\"bleu_values\"]=bleus_reference_data\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    bleus_data = calculate_bleus(config)\n",
    "    results[config][\"bleu_values\"]=bleus_data\n",
    "    plot_bleu_violinplot(bleus_data,bleus_reference_data,config)\n",
    "    plot_bleu_boxplot(bleus_data,bleus_reference_data,config)\n",
    "    plot_bleu_histogram(bleus_data,bleus_reference_data,config)\n",
    "    del bleus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "Before the samples can be inspected, the items need to be re-indexed. \n",
    "While all config_results are in the reference_results, there might is an issue with the data being shuffeld. \n",
    "\n",
    "To fix this, a reindexing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Reindexing Pseudocode\n",
    "\n",
    "def lookup_index(sentence, comparison_dict):\n",
    "    for (gold_key,gold_value) in comparison_dict.items():\n",
    "        if sentence == gold_value:\n",
    "            return gold_key\n",
    "    return -1\n",
    "\n",
    "# For each config (that is not reference)\n",
    "    # Create a lookup of reference_gold_index -> config_gold_index\n",
    "    # Invert the lookup \n",
    "    # Make a new dictionary where\n",
    "        # For every key of the config_gold\n",
    "        # lookup the key of the reference_gold\n",
    "        # And fill it with {reference_gold_key : config_gold_value}\n",
    "        # Do the same with the non-gold results\n",
    "        # Fill it with {reference_gold_key : config_result_value}\n",
    "    # Set result[config_X][\"gold_results\"] to the newly created, matching index one \n",
    "    # same for non-gold-results\n",
    "    \n",
    "for config in non_reference_configs:\n",
    "    keyMapping={}\n",
    "    for (k,v) in results[config][\"gold_results\"].items():\n",
    "        gk = lookup_index(v,results[\"reference\"][\"gold_results\"])\n",
    "        keyMapping[k]=gk\n",
    "    new_gold_results={}\n",
    "    new_results={}\n",
    "    for (config_key,gold_key) in keyMapping.items():\n",
    "        if gold_key != -1:\n",
    "            new_gold_results[gold_key]=results[config][\"gold_results\"][config_key]\n",
    "            new_results[gold_key]=results[config][\"results\"][config_key]\n",
    "    results[config][\"gold_results\"]=new_gold_results\n",
    "    results[config][\"results\"]=new_results\n",
    "    #print(config,keyMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 250\n",
    "print(results[\"reference\"][\"gold_results\"][sample_index] )\n",
    "#print(results[\"config_2\"][\"gold_results\"][sample_index])\n",
    "print()\n",
    "print(results[\"reference\"][\"results\"][sample_index])\n",
    "print(results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hall of Shame** \n",
    "\n",
    "worst entries in terms of t-score (either one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "biggest_len_inc = 0\n",
    "biggest_len_inc_pos = ()\n",
    "\n",
    "biggest_len_dec = 0\n",
    "biggest_len_dec_pos = ()\n",
    "\n",
    "biggest_jaccard_dist = 0\n",
    "biggest_jaccard_dist_pos = ()\n",
    "\n",
    "smallest_jaccard_dist = 1 \n",
    "smallest_jaccard_dist_pos = ()\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        \n",
    "        if len(reference)-len(altered)>biggest_len_inc:\n",
    "            biggest_len_inc = len(reference)-len(altered)\n",
    "            biggest_len_inc_pos = (index,config)\n",
    "        if len(altered)-len(reference)>biggest_len_dec:\n",
    "            biggest_len_dec = len(altered)-len(reference)\n",
    "            biggest_len_dec_pos = (index,config)\n",
    "            \n",
    "        jacc_dist = jaccard_wrapper(altered,reference)\n",
    "        if jacc_dist > biggest_jaccard_dist and jacc_dist < 1:\n",
    "            biggest_jaccard_dist = jacc_dist\n",
    "            biggest_jaccard_dist_pos = (index,config)\n",
    "        if jacc_dist < smallest_jaccard_dist and jacc_dist > 0:\n",
    "            smallest_jaccard_dist = jacc_dist\n",
    "            smallest_jaccard_dist_pos = (index,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_item_with_reference(index,config):\n",
    "    print(\"Gold:\")\n",
    "    print(results[config][\"gold_results\"][index])\n",
    "    print(\"Reference:\")\n",
    "    print(results[\"reference\"][\"results\"][index])\n",
    "    print(\"Altered:\")\n",
    "    print(results[config][\"results\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Biggest jaccard Distance (that is not 1):\\n\")\n",
    "print_config_item_with_reference(biggest_jaccard_dist_pos[0],biggest_jaccard_dist_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Biggest decrease in length:\\n\")\n",
    "print_config_item_with_reference(biggest_len_inc_pos[0],biggest_len_inc_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Biggest increase in length:\\n\")\n",
    "print_config_item_with_reference(biggest_len_dec_pos[0],biggest_len_dec_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Smallest Jaccard Distance (that is not 0):\\n\")\n",
    "print_config_item_with_reference(smallest_jaccard_dist_pos[0],smallest_jaccard_dist_pos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fishy Example from a Kids Java-Learning Book. \n",
    "Code is actually about learning switch-case statements and set a image to the corresponding fishes (e.g. empty fish glass, fish glass with 2 fishes etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishyKey = -1\n",
    "for (key,value) in results[\"reference\"][\"gold_results\"].items():\n",
    "    #print(value)\n",
    "    if \"makeAFishyDecision \" in value:\n",
    "        fishyKey = key\n",
    "\n",
    "print(\"Fishy Results! \\n\")\n",
    "print(\"Gold:\")\n",
    "print(results[\"reference\"][\"gold_results\"][fishyKey])\n",
    "print(\"Reference:\")\n",
    "print(results[\"reference\"][\"results\"][fishyKey],\"\\n\")\n",
    "#for config in non_reference_configs:\n",
    "for config in [\"config_0\",\"config_1\",\"config_20\",\"config_10\"]:\n",
    "    print(f\"Altered({config},{print_archetype_info(config)}):\")\n",
    "    print(results[config][\"results\"][fishyKey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method requires for the xs and ys to be sorted! \n",
    "Without matching indizes it does not make any sense.\n",
    "\"\"\"\n",
    "def calculate_jaccard_distances(xs,ys,ngrams=1):\n",
    "    agg = []\n",
    "    indX = len(xs)\n",
    "    indY = len(ys)\n",
    "    if indX != indY:\n",
    "        raise IndexError()\n",
    "    else:\n",
    "        running_index = 0\n",
    "        while running_index < indX:\n",
    "            agg.append(jaccard_wrapper(xs[running_index],ys[running_index],ngrams))\n",
    "            running_index = running_index + 1\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jaccs = {}\n",
    "for config in non_reference_configs:\n",
    "    distances = calculate_jaccard_distances(results[\"reference\"][\"results\"],results[config][\"results\"])\n",
    "    \n",
    "    jaccs[config]=distances\n",
    "    \n",
    "    sns.displot(\n",
    "        distances,\n",
    "        kind=\"hist\", kde=True,\n",
    "        bins=20,\n",
    "        height=6, aspect=10/6)\n",
    "    plt.title(f\"Histogram of JaccardDistances for {config}\\n({print_archetype_info(config)})\")\n",
    "    plt.xlabel(\"JaccardDistance \\n Reference to Altered\")\n",
    "    plt.ylabel(\"# of Entries\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,10000)\n",
    "    \n",
    "    plt.savefig(f'images/{config}_jaccard_histogram.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,9))\n",
    "\n",
    "violin_parts = plt.violinplot(\n",
    "    dataset=jaccs.values()\n",
    "    ,showextrema=False\n",
    "    ,points=35\n",
    ")\n",
    "\n",
    "archetype_color = {\n",
    "    'add-neutral':plt.cm.get_cmap('PuBuGn')(0.1),\n",
    "    'add-var(pseudo)':plt.cm.get_cmap('PuBuGn')(0.25),\n",
    "    'add-var(random)':plt.cm.get_cmap('PuBuGn')(0.4),\n",
    "    'if-true':plt.cm.get_cmap('PuBuGn')(0.5),\n",
    "    'if-true & add-neutral':plt.cm.get_cmap('PuBuGn')(0.65),\n",
    "    'mixed names(pseudo)':plt.cm.get_cmap('PuBuGn')(0.8),\n",
    "    'mixed-names(random)':plt.cm.get_cmap('PuBuGn')(1.0)\n",
    "}\n",
    "\n",
    "part_runner = 0\n",
    "\n",
    "while part_runner < len(violin_parts['bodies']):\n",
    "    pc = violin_parts['bodies'][part_runner]\n",
    "    con = list(jaccs.keys())[part_runner]\n",
    "    pc.set_facecolor(archetype_color[config_archetypes[con]])\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_label(config_archetypes[con])\n",
    "    part_runner = part_runner +1 \n",
    "    \n",
    "plt.title(f\"ViolinPlot of Jaccard_Distances\")\n",
    "plt.ylabel(\"Jaccard Distance\")\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.xticks(\n",
    "    range(1,len(jaccs.keys())+1)\n",
    "    ,labels=[f\"{config}\\n{print_archetype_info(config)}\" for config in jaccs.keys()]\n",
    ")\n",
    "\n",
    "#plt.legend()\n",
    "\n",
    "plt.savefig(f'images/jaccard_distances_violinplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,12))\n",
    "jacc_df=pd.DataFrame.from_dict(jaccs)\n",
    "\n",
    "sns.boxplot(\n",
    "    data=jacc_df)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"Boxplot of Jaccard_Distances\")\n",
    "plt.ylabel(\"Jaccard Distance\")\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(f'images/jaccard_distances_boxplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Scores\n",
    "\n",
    "from the paper, we provide two ways of checking on the transformation score: \n",
    "\n",
    "```\n",
    "delta_tscore = bleu(gold,reference) - bleu(gold,altered)\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```\n",
    "tquot = bleu(gold,altered) / bleu(gold,reference)\n",
    "```\n",
    "\n",
    "which we can perfectly write as python and see how they are doing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_tscore = lambda gold,reference,altered : bleu_wrapper(reference,gold) - bleu_wrapper(altered,gold)\n",
    "\n",
    "tquot = lambda gold,reference,altered : bleu_wrapper(altered,gold) / bleu_wrapper(reference,gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_tscore(results[\"config_2\"][\"gold_results\"][sample_index],results[\"reference\"][\"results\"][sample_index],results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tquot(results[\"config_2\"][\"gold_results\"][sample_index],results[\"reference\"][\"results\"][sample_index],results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 10\n",
    "worst_n = []\n",
    "best_n = []\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        try:\n",
    "            r = (delta_tscore(gold,reference,altered),config,index)\n",
    "        except ZeroDivisionError: \n",
    "            print(\"Error at \",index,config)\n",
    "        else:\n",
    "            running_agg.append(r)\n",
    "            \n",
    "    running_agg = sorted(running_agg + worst_n + best_n,key=lambda x:x[0])\n",
    "    worst_n=running_agg[:n]\n",
    "    best_n=running_agg[-n:]\n",
    "    del running_agg\n",
    "    #print(\"ckecked for worst in \",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (diff,config,index) in worst_n+best_n:\n",
    "    print(\n",
    "        f\"delta_tscore {round(diff,4)}, {config}@{index}\\n\",\n",
    "        f'gold: \\t {results[\"reference\"][\"gold_results\"][index]} \\n',\n",
    "        f'ref: \\t {results[\"reference\"][\"results\"][index]}\\n',\n",
    "        f'alt: \\t {results[config][\"results\"][index]}\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m = 10\n",
    "worst_m = []\n",
    "best_m = []\n",
    "\n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = (tquot(gold,reference,altered),config,index)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            #there are quite a lot of errors \n",
    "            r = (0,config,index)\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    running_agg = sorted(running_agg + worst_m + best_m,key=lambda x:x[0])\n",
    "    worst_m=running_agg[:m]\n",
    "    best_m=running_agg[-m:]\n",
    "    del running_agg\n",
    "    #print(\"ckecked for worst in \",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (diff,config,index) in worst_n+best_n:\n",
    "    print(\n",
    "        f\"tquot {round(diff,4)}, {config}@{index}\\n\",\n",
    "        f'gold: \\t {results[\"reference\"][\"gold_results\"][index]} \\n',\n",
    "        f'ref: \\t {results[\"reference\"][\"results\"][index]}\\n',\n",
    "        f'alt: \\t {results[config][\"results\"][index]}\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of TScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "delta_tscore_data = []\n",
    "quot_tscore_data = []\n",
    "\n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    delta_running_agg = []\n",
    "    quot_running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        a = ()\n",
    "        b = ()\n",
    "        try:\n",
    "            a = delta_tscore(gold,reference,altered)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            a = 0\n",
    "        delta_running_agg.append(a)\n",
    "        try:\n",
    "            b = tquot(gold,reference,altered)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            b = 0\n",
    "        \n",
    "        quot_running_agg.append(b)    \n",
    "    delta_tscore_data.append(delta_running_agg)\n",
    "    quot_tscore_data.append(quot_running_agg)\n",
    "    results[config][\"quot-tscores\"]=quot_tscore_data\n",
    "    results[config][\"delta-tscores\"]=delta_tscore_data\n",
    "    del quot_running_agg\n",
    "    del delta_running_agg\n",
    "    \n",
    "filtered_quot_tscore_data = [[a for a in x if a<2] for x in quot_tscore_data]\n",
    "#filtered_delta_tscore_data = [[a for a in x if a != 0] for x in delta_tscore_data]\n",
    "filtered_delta_tscore_data = [[a for a in x if abs(a)>0.025] for x in delta_tscore_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 7))\n",
    "\n",
    "filtered_delta_tscore_df = pd.DataFrame(filtered_delta_tscore_data )\n",
    "filtered_delta_tscore_df = filtered_delta_tscore_df.transpose()\n",
    "filtered_delta_tscore_df.columns=(list(non_reference_configs))\n",
    "\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()\n",
    "# plot_data = delta_tscore_data \n",
    "plot_data = filtered_delta_tscore_data \n",
    "\n",
    "# plot violin plot\n",
    "sns.violinplot(data=filtered_delta_tscore_df,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "plt.title('Distribution of non-null delta-tscores')\n",
    "\n",
    "plt.savefig(f'images/delta_tscore_violins_by_config.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(21, 7))\n",
    "\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()\n",
    "plt.title('Distribution of non-null delta-tscores \\n by archetype ')\n",
    "\n",
    "sns.boxplot(data=filtered_delta_tscore_df)\n",
    "\n",
    "    \n",
    "plt.savefig(f'images/delta_tscore_boxplots_by_config.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 7))\n",
    "\n",
    "filtered_quot_tscore_df = pd.DataFrame(filtered_quot_tscore_data )\n",
    "filtered_quot_tscore_df = filtered_quot_tscore_df.transpose()\n",
    "filtered_quot_tscore_df.columns=(list(non_reference_configs))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "# plot violin plot\n",
    "sns.violinplot(data=filtered_quot_tscore_df,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "plt.title('Distribution of quot-tscores')\n",
    "\n",
    "plt.savefig(f'images/quot_tscore_violins_by_config.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(21, 7))\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Distribution of quot-tscores \\n by archetype ')\n",
    "\n",
    "sns.boxplot(data=filtered_quot_tscore_df)\n",
    "\n",
    "    \n",
    "plt.savefig(f'images/quot_tscore_boxplots_by_config.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Score by archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_delta_tscore_data = {}\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    if not config_archetypes[config] in archetype_delta_tscore_data.keys():\n",
    "        archetype_delta_tscore_data[config_archetypes[config]] = []\n",
    "    else:\n",
    "        running_agg = archetype_delta_tscore_data[config_archetypes[config]]\n",
    "    running_agg = running_agg + results[config][\"delta-t-scores\"]\n",
    "            \n",
    "    archetype_delta_tscore_data[config_archetypes[config]] = running_agg\n",
    "    del running_agg\n",
    "\n",
    "#archetype_filtered_delta_tscore_data = [[a for a in x if a != 0] for x in delta_tscore_data]\n",
    "archetype_filtered_delta_tscore_data = [[a for a in x if abs(a)>0.025] for x in archetype_delta_tscore_data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "delta_tscore_df = pd.DataFrame(archetype_delta_tscore_data.values() )\n",
    "delta_tscore_df = delta_tscore_df.transpose()\n",
    "delta_tscore_df.columns=(list(all_archetypes))\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()\n",
    "plt.title('Distribution of delta-tscores \\n by archetype ')\n",
    "\n",
    "sns.violinplot(data=delta_tscore_df)\n",
    "\n",
    "    \n",
    "plt.savefig(f'images/delta_tscore_violins_by_archetype.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()\n",
    "plt.title('Distribution of delta-tscores \\n by archetype ')\n",
    "\n",
    "sns.boxplot(data=delta_tscore_df)\n",
    "\n",
    "    \n",
    "plt.savefig(f'images/delta_tscore_boxplots_by_archetype.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "non_null_delta_df = pd.DataFrame(archetype_filtered_delta_tscore_data)\n",
    "\n",
    "non_null_delta_df = non_null_delta_df.transpose()\n",
    "non_null_delta_df.columns=(list(all_archetypes))\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()\n",
    "plt.title('Distribution of non-null delta-tscores \\n by archetype ')\n",
    "\n",
    "sns.violinplot(data=non_null_delta_df)\n",
    "\n",
    "    \n",
    "plt.savefig(f'images/nonnull_delta_tscore_violins_by_archetype.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()\n",
    "plt.title('Distribution of non-null delta-tscores \\n by archetype ')\n",
    "\n",
    "sns.boxplot(data=non_null_delta_df)\n",
    "\n",
    "    \n",
    "plt.savefig(f'images/nonnull_delta_tscore_boxplots_by_archetype.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Robustness\n",
    "\n",
    "the t-robustness is the average of tscores on a configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        try:\n",
    "            running_agg.append(delta_tscore(gold,reference,altered))\n",
    "        except ZeroDivisionError: \n",
    "            print(\"Error at \",index,config)\n",
    "    \n",
    "    robustness = np.mean(running_agg)\n",
    "    variance = np.var(running_agg)\n",
    "    #print(config,\"delta-trobustness\",robustness)\n",
    "    #print(config,\"delta-t-variance\",variance)\n",
    "    results[config][\"delta-t-robustness\"]=robustness\n",
    "    results[config][\"delta-t-variance\"]=variance\n",
    "    del running_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        try:\n",
    "            running_agg.append(tquot(gold,reference,altered))\n",
    "        except ZeroDivisionError: \n",
    "            running_agg.append(0.0)\n",
    "    \n",
    "    robustness = np.mean(running_agg)\n",
    "    variance = np.var(running_agg)\n",
    "    #print(config,\"quot-trobustness\",robustness)\n",
    "    #print(config,\"quot-t-variance\",variance)\n",
    "    results[config][\"quot-t-robustness\"]=robustness\n",
    "    results[config][\"quot-t-variance\"]=variance\n",
    "    del running_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    robustness_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        robustness_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"delta-t-robustness\"]\n",
    "   \n",
    "robustness_data_df = pd.DataFrame.from_dict(robustness_data)\n",
    "\n",
    "plt.title(\"TRobustness given delta-tscore\")\n",
    "plt.ylabel(\"Delta-T-Robustness\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(robustness_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(robustness_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    robustness_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        robustness_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"quot-t-robustness\"]\n",
    "   \n",
    "robustness_data_df = pd.DataFrame.from_dict(robustness_data)\n",
    "\n",
    "plt.title(\"TRobustness given quot-tscore\")\n",
    "plt.ylabel(\"Quot-T-Robustness\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(robustness_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(robustness_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    variance_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        variance_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"delta-t-variance\"]\n",
    "   \n",
    "variance_data_df = pd.DataFrame.from_dict(variance_data)\n",
    "\n",
    "plt.title(\"TVariance given delta-tscore\")\n",
    "plt.ylabel(\"Delta-T-Variance\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(variance_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(variance_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variance_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    variance_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        variance_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"quot-t-variance\"]\n",
    "   \n",
    "variance_data_df = pd.DataFrame.from_dict(variance_data)\n",
    "\n",
    "plt.title(\"TVariance given quot-tscore\")\n",
    "plt.ylabel(\"quot-T-Variance\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(variance_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(variance_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences\n",
    "\n",
    "Looking for Differences in results - similar to Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    entries = len(results[config][\"results\"].keys())\n",
    "    diffs = 0\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        \n",
    "        if reference != altered:\n",
    "            diffs = diffs + 1 \n",
    "            \n",
    "    results[config][\"diffs\"]=diffs\n",
    "    #print(f\"{config} had {diffs} differences {round(float(diffs)/entries*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_df_data = [(c,results[c][\"diffs\"],config_archetypes[c]) for c in non_reference_configs ]\n",
    "    \n",
    "diffs_df = pd.DataFrame(diffs_df_data)\n",
    "diffs_df.columns=[\"config\",\"diffs\",\"archetype\"]\n",
    "\n",
    "diffs_df= diffs_df.sort_values(by=[\"archetype\", \"config\"])\n",
    "\n",
    "plt.figure(figsize=(21, 7))\n",
    "plt.grid()\n",
    "\n",
    "plt.title('Result-Differences per Configuration')\n",
    "\n",
    "\n",
    "sns.barplot(x=\"config\",y=\"diffs\",data=diffs_df,hue=\"archetype\")\n",
    "\n",
    "\n",
    "plt.savefig(f'images/number_of_diffs_by_config.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
