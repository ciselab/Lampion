{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CodeBert Grid Experiment Evaluation\n",
    "\n",
    "Nice to see you around! Have a seat.\n",
    "Would you like a drink? Maybe a cigar?\n",
    "\n",
    "Make sure to have all required dependencies installed - they are listed in the [environment.yml](./environment.yml). \n",
    "You create a conda environment from the yml using \n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate Lampion-Codebert-Evaluation\n",
    "```\n",
    "\n",
    "Make sure to run your Jupyter Notebook from that environment! \n",
    "Otherwise you are (still) missing the dependencies. \n",
    "\n",
    "**OPTIONALLY** you can use the environment in which your jupter notebook is already running, with starting a new terminal (from jupyter) and run \n",
    "\n",
    "```\n",
    "conda env update --prefix ./env --file environment.yml  --prune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "# Homebrew Imports (python-file next to this)\n",
    "import bleu_evaluator as foreign_bleu\n",
    "\n",
    "# Set Jupyter vars\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data-Loading / Preparation\n",
    "\n",
    "Make sure that your dataset looks like described in the [Readme](./README.md), that is \n",
    "\n",
    "```\n",
    "./data\n",
    "    /GridExp_XY\n",
    "        /configs\n",
    "            /reference\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_0\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_1\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "    ...\n",
    "```\n",
    "\n",
    "where the configs **must** be numbered to be correctly detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs the bleu-score upon the config files, creating the bleu.txt's \n",
    "# If your data package was provided including the txt you dont need to do this. \n",
    "# Existing bleu.txt's will be overwritten. \n",
    "\n",
    "#!./metric_runner.sh ./data/PreliminaryResults/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_directory = \"./data/PreliminaryResults\"\n",
    "\n",
    "# These archetypes are later used to group the configurations\n",
    "# While to grouping is up to you, right here it simply is one archetype for each transformation type, \n",
    "# Grouping together different configs with the same transformations applied (but different #Transformations)\n",
    "config_archetypes = {\n",
    "    \"config_0\":\"if-true\",\"config_1\":\"if-true\",\"config_2\":\"if-true\",\n",
    "    \"config_3\":\"mixed names(pseudo)\",\"config_4\":\"mixed names(pseudo)\",\"config_5\":\"mixed names(pseudo)\",\n",
    "    \"config_6\":\"add-neutral\",\"config_7\":\"add-neutral\",\"config_8\":\"add-neutral\",\n",
    "    \"config_9\":\"mixed-names(random)\",\"config_10\":\"mixed-names(random)\",\"config_11\":\"mixed-names(random)\",\n",
    "    \"config_12\": \"add-var(pseudo)\",\"config_13\": \"add-var(pseudo)\",\"config_14\": \"add-var(pseudo)\",\n",
    "    \"config_15\": \"add-var(random)\",\"config_16\": \"add-var(random)\",\"config_17\": \"add-var(random)\",\n",
    "    \"config_18\": \"if-true & add-neutral\",\"config_19\": \"if-true & add-neutral\",\"config_20\": \"if-true & add-neutral\"\n",
    "}\n",
    "\n",
    "print(f\"looking for results in {data_directory}\" )\n",
    "\n",
    "results={}\n",
    "\n",
    "for root,dirs,files in os.walk(data_directory):\n",
    "    for name in files:\n",
    "        if \".gold\" in name:\n",
    "            directory = os.path.basename(root)\n",
    "            results[directory]={}\n",
    "            \n",
    "            results[directory][\"result_file\"]=os.path.join(root,\"test_0.output\")\n",
    "            results[directory][\"gold_file\"]=os.path.join(root,\"test_0.gold\")\n",
    "            results[directory][\"bleu_file\"]=os.path.join(root,\"bleu.txt\")\n",
    "            if os.path.exists(os.path.join(root,\"config.properties\")):\n",
    "                results[directory][\"property_file\"]=os.path.join(root,\"config.properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    \"\"\"\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    \"\"\"\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "print(\"reading in property-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    if \"property_file\" in results[key].keys():\n",
    "        results[f\"{key}\"][\"properties\"]=load_properties(results[key][\"property_file\"])\n",
    "\n",
    "print(\"done reading the properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in result-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    result_file = results[key][\"result_file\"]\n",
    "    f = open(result_file)\n",
    "    lines=f.readlines()\n",
    "    results[key][\"results\"]={}\n",
    "    for l in lines:\n",
    "        num = int(l.split(\"\\t\")[0])\n",
    "        content = l.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"results\"][num] = content\n",
    "    f.close()\n",
    "    \n",
    "    gold_file = results[key]['gold_file']\n",
    "    gf = open(gold_file)\n",
    "    glines=gf.readlines()\n",
    "    results[key][\"gold_results\"]={}\n",
    "    for gl in glines:\n",
    "        num = int(gl.split(\"\\t\")[0])\n",
    "        content = gl.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"gold_results\"][num] = content\n",
    "    gf.close()\n",
    "\n",
    "print(\"done reading the result files\")\n",
    "# Comment this in for inspection of results\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in the bleu-scores\")\n",
    "\n",
    "for key in results.keys():\n",
    "    bleu_file = results[key][\"bleu_file\"]\n",
    "    f = open(bleu_file)\n",
    "    score=f.readlines()[0]\n",
    "    results[key][\"bleu\"]=float(score)\n",
    "    f.close()\n",
    "    \n",
    "print(\"done reading the bleu-scores\")\n",
    "\n",
    "#results[\"config_0\"][\"bleu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in results.keys():\n",
    "    if \"property_file\" in results[key].keys():\n",
    "        results[key][\"archetype\"]=config_archetypes[key]\n",
    "        \n",
    "non_reference_configs = [key for key in results.keys() if \"reference\" != key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_wrapper(sentenceA,sentenceB,ngram=1):\n",
    "    tokensA = nltk.word_tokenize(sentenceA)\n",
    "    tokensB = nltk.word_tokenize(sentenceB)\n",
    "\n",
    "    ngA_tokens = set(nltk.ngrams(tokensA, n=ngram))\n",
    "    ngB_tokens = set(nltk.ngrams(tokensB, n=ngram))\n",
    "    \n",
    "    return nltk.jaccard_distance(ngA_tokens, ngB_tokens)\n",
    "\n",
    "def bleu_wrapper(sentence_to_check,reference):\n",
    "    check_tokens = nltk.word_tokenize(sentence_to_check)\n",
    "    ref_tokens = nltk.word_tokenize(reference)\n",
    "    \n",
    "    # From comparing the foreign_bleu and nltk the method4 seems to match\n",
    "    # The Paper mentiones the BLEU-4-Score with a citation to chen & cherry\n",
    "    # I wish I could be named chen & cherry, its a very cool name. \n",
    "    chencherry = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    smooth_fn = chencherry.method4\n",
    "    \n",
    "    try:\n",
    "        return nltk.translate.bleu_score.sentence_bleu([ref_tokens],check_tokens,smoothing_function=smooth_fn)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "bleu_wrapper(results[\"config_2\"][\"results\"][sample_index],results[\"config_2\"][\"gold_results\"][sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bleu-Scores\n",
    "\n",
    "In the following, the BLEU-scores will be calculated using the foreign libary. \n",
    "While there have been minor changes to standard-BLEU, it is the same as used in the original experiment.\n",
    "\n",
    "The aggregated BLEU-Scores will be stored to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    bleu_data[archetype]={}\n",
    "    bleu_data[archetype][0]=results[\"reference\"][\"bleu\"]\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        bleu_data[archetype][int(results[c][\"properties\"][\"transformations\"])]=results[c][\"bleu\"]\n",
    "   \n",
    "bleu_data_df = pd.DataFrame.from_dict(bleu_data)\n",
    "#bleu_data_df = bleu_data_df.sort_index() #(by=\"index\")\n",
    "bleu_data_df = bleu_data_df.sort_index()\n",
    "bleu_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"BLEU-Score\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14,7)\n",
    "\n",
    "plot = plt.plot(bleu_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(bleu_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleus = lambda config_id : [\n",
    "    bleu_wrapper(results[config_id][\"results\"][i],results[config_id][\"gold_results\"][i]) \n",
    "    for i \n",
    "    in results[config_id][\"results\"].keys()\n",
    "]\n",
    "\n",
    "\n",
    "sample_bleus_config_data = calculate_bleus(\"config_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bleu_histogram(config_data,reference_data,title=None):\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.hist([reference_data,config_data],bins=20)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(f\"Histogram of Bleu-Scores for {title}\")\n",
    "        plt.legend([\"reference\",title])\n",
    "    plt.xlabel(\"Bleu-Score\")\n",
    "    plt.ylabel(\"# of Entries\")\n",
    "    plt.xlim(0,1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_bleu_boxplot(config_data,reference_data,title=None):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.boxplot([reference_data,config_data])\n",
    "    \n",
    "    if title:\n",
    "        plt.title(f\"Boxplot of Bleu-Scores for {title}\")\n",
    "        plt.legend([\"reference\",title])\n",
    "    plt.xlabel(\"Bleu-Score\")\n",
    "    plt.ylabel(\"# of Entries\")\n",
    "    #plt.xlim(0,1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_bleu_violinplot(config_data,reference_data,title=None):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.violinplot(\n",
    "        dataset=[reference_data,config_data]\n",
    "        #,quantiles=[0.25,0.5,0.75]\n",
    "        ,vert=False\n",
    "    )\n",
    "    \n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(f\"ViolinPlot of Bleu-Scores for {title}\")\n",
    "        plt.yticks([1,2],labels=[\"Reference\",title])\n",
    "    else:\n",
    "        plt.yticks([1,2],labels=[\"Reference\",\"Other\"])\n",
    "    plt.xlabel(\"Bleu-Score\")\n",
    "    plt.xlim(0.01,1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_bleu_violinplot(bleus_config_data,bleus_reference_data,\"config_20\")\n",
    "plot_bleu_histogram(bleus_config_data,bleus_reference_data,\"config_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bleus_reference_data = calculate_bleus(\"reference\")\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    bleus_data = calculate_bleus(config)\n",
    "    plot_bleu_violinplot(bleus_data,bleus_reference_data,config)\n",
    "    plot_bleu_histogram(bleus_data,bleus_reference_data,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "Before the samples can be inspected, the items need to be re-indexed. \n",
    "While all config_results are in the reference_results, there might is an issue with the data being shuffeld. \n",
    "\n",
    "To fix this, a reindexing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Reindexing Pseudocode\n",
    "\n",
    "def lookup_index(sentence, comparison_dict):\n",
    "    for (gold_key,gold_value) in comparison_dict.items():\n",
    "        if sentence == gold_value:\n",
    "            return gold_key\n",
    "    return -1\n",
    "\n",
    "# For each config (that is not reference)\n",
    "    # Create a lookup of reference_gold_index -> config_gold_index\n",
    "    # Invert the lookup \n",
    "    # Make a new dictionary where\n",
    "        # For every key of the config_gold\n",
    "        # lookup the key of the reference_gold\n",
    "        # And fill it with {reference_gold_key : config_gold_value}\n",
    "        # Do the same with the non-gold results\n",
    "        # Fill it with {reference_gold_key : config_result_value}\n",
    "    # Set result[config_X][\"gold_results\"] to the newly created, matching index one \n",
    "    # same for non-gold-results\n",
    "    \n",
    "for config in non_reference_configs:\n",
    "    keyMapping={}\n",
    "    for (k,v) in results[config][\"gold_results\"].items():\n",
    "        gk = lookup_index(v,results[\"reference\"][\"gold_results\"])\n",
    "        keyMapping[k]=gk\n",
    "    new_gold_results={}\n",
    "    new_results={}\n",
    "    for (config_key,gold_key) in keyMapping.items():\n",
    "        if gold_key != -1:\n",
    "            new_gold_results[gold_key]=results[config][\"gold_results\"][config_key]\n",
    "            new_results[gold_key]=results[config][\"results\"][config_key]\n",
    "    results[config][\"gold_results\"]=new_gold_results\n",
    "    results[config][\"results\"]=new_results\n",
    "    #print(config,keyMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 250\n",
    "print(results[\"reference\"][\"gold_results\"][sample_index] )\n",
    "#print(results[\"config_2\"][\"gold_results\"][sample_index])\n",
    "print()\n",
    "print(results[\"reference\"][\"results\"][sample_index])\n",
    "print(results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hall of Shame** \n",
    "\n",
    "worst entries in terms of t-score (either one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_len_diff = 0\n",
    "biggest_len_diff_pos = ()\n",
    "\n",
    "\n",
    "biggest_jaccard_dist = 0\n",
    "biggest_jaccard_dist_pos = ()\n",
    "\n",
    "smallest_jaccard_dist = 1 \n",
    "smallest_jaccard_dist_pos = ()\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        \n",
    "        if abs(len(reference)-len(altered))>biggest_len_diff:\n",
    "            biggest_len_diff = abs(len(reference)-len(altered))\n",
    "            biggest_len_diff_pos = (index,config)\n",
    "\n",
    "        jacc_dist = jaccard_wrapper(altered,reference)\n",
    "        if jacc_dist > biggest_jaccard_dist and jacc_dist < 1:\n",
    "            biggest_jaccard_dist = jacc_dist\n",
    "            biggest_jaccard_dist_pos = (index,config)\n",
    "        if jacc_dist < smallest_jaccard_dist and jacc_dist > 0:\n",
    "            smallest_jaccard_dist = jacc_dist\n",
    "            smallest_jaccard_dist_pos = (index,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_item_with_reference(index,config):\n",
    "    print(\"Gold:\")\n",
    "    print(results[config][\"gold_results\"][index])\n",
    "    print(\"Reference:\")\n",
    "    print(results[\"reference\"][\"results\"][index])\n",
    "    print(\"Altered:\")\n",
    "    print(results[config][\"results\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Biggest jaccard Distance (that is not 1):\")\n",
    "print_config_item_with_reference(biggest_jaccard_dist_pos[0],biggest_jaccard_dist_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Biggest difference in length:\")\n",
    "print_config_item_with_reference(biggest_len_diff_pos[0],biggest_len_diff_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Smallest Jaccard Distance (that is not 0):\")\n",
    "print_config_item_with_reference(smallest_jaccard_dist_pos[0],smallest_jaccard_dist_pos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Scores\n",
    "\n",
    "from the paper, we provide two ways of checking on the transformation score: \n",
    "\n",
    "```\n",
    "delta_tscore = bleu(gold,reference) - bleu(gold,altered)\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```\n",
    "tquot = bleu(gold,altered) / bleu(gold,reference)\n",
    "```\n",
    "\n",
    "which we can perfectly write as python and see how they are doing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_tscore = lambda gold,reference,altered : bleu_wrapper(reference,gold) - bleu_wrapper(altered,gold)\n",
    "\n",
    "tquot = lambda gold,reference,altered : bleu_wrapper(altered,gold) / bleu_wrapper(reference,gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_tscore(results[\"config_2\"][\"gold_results\"][sample_index],results[\"reference\"][\"results\"][sample_index],results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tquot(results[\"config_2\"][\"gold_results\"][sample_index],results[\"reference\"][\"results\"][sample_index],results[\"config_2\"][\"results\"][sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 10\n",
    "worst_n = []\n",
    "best_n = []\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        try:\n",
    "            r = (delta_tscore(gold,reference,altered),config,index)\n",
    "        except ZeroDivisionError: \n",
    "            print(\"Error at \",index,config)\n",
    "        else:\n",
    "            running_agg.append(r)\n",
    "            \n",
    "    running_agg = sorted(running_agg + worst_n + best_n,key=lambda x:x[0])\n",
    "    worst_n=running_agg[:n]\n",
    "    best_n=running_agg[-n:]\n",
    "    del running_agg\n",
    "    #print(\"ckecked for worst in \",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (diff,config,index) in worst_n+best_n:\n",
    "    print(\n",
    "        f\"delta_tscore {round(diff,4)}, {config}@{index}\\n\",\n",
    "        f'gold: \\t {results[\"reference\"][\"gold_results\"][index]} \\n',\n",
    "        f'ref: \\t {results[\"reference\"][\"results\"][index]}\\n',\n",
    "        f'alt: \\t {results[config][\"results\"][index]}\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m = 10\n",
    "worst_m = []\n",
    "best_m = []\n",
    "\n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = (tquot(gold,reference,altered),config,index)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            #there are quite a lot of errors \n",
    "            r = (0,config,index)\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    running_agg = sorted(running_agg + worst_m + best_m,key=lambda x:x[0])\n",
    "    worst_m=running_agg[:m]\n",
    "    best_m=running_agg[-m:]\n",
    "    del running_agg\n",
    "    #print(\"ckecked for worst in \",config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (diff,config,index) in worst_n+best_n:\n",
    "    print(\n",
    "        f\"tquot {round(diff,4)}, {config}@{index}\\n\",\n",
    "        f'gold: \\t {results[\"reference\"][\"gold_results\"][index]} \\n',\n",
    "        f'ref: \\t {results[\"reference\"][\"results\"][index]}\\n',\n",
    "        f'alt: \\t {results[config][\"results\"][index]}\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of TScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "delta_tscore_data = []\n",
    "\n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = delta_tscore(gold,reference,altered)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            r = 0\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    delta_tscore_data.append(running_agg)\n",
    "    del running_agg\n",
    "\n",
    "#filtered_delta_tscore_data = [[a for a in x if a != 0] for x in delta_tscore_data]\n",
    "filtered_delta_tscore_data = [[a for a in x if abs(a)>0.025] for x in delta_tscore_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n",
    "\n",
    "# plot_data = delta_tscore_data \n",
    "plot_data = filtered_delta_tscore_data \n",
    "\n",
    "# plot violin plot\n",
    "axs[0].violinplot(plot_data,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "axs[0].set_title('Distribution of delta-tscores')\n",
    "\n",
    "# plot box plot\n",
    "axs[1].boxplot(plot_data)\n",
    "axs[1].set_title('Distribution of delta-tscores')\n",
    "\n",
    "# add x-tick labels\n",
    "plt.setp(axs, xticks=[y + 1 for y  in range(21)]\n",
    "        # ,xticklabels=['x1', 'x2', 'x3', 'x4']\n",
    "        )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quot_tscore_data = []\n",
    "\n",
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = tquot(gold,reference,altered)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            r = 0\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    quot_tscore_data.append(running_agg)\n",
    "    del running_agg\n",
    "    \n",
    "filtered_quot_tscore_data = [[a for a in x if a<2] for x in quot_tscore_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n",
    "\n",
    "# plot_data = quot_tscore_data \n",
    "plot_data = filtered_quot_tscore_data \n",
    "\n",
    "# plot violin plot\n",
    "axs[0].violinplot(plot_data,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "axs[0].set_title('Distribution of quot-tscores')\n",
    "\n",
    "# plot box plot\n",
    "axs[1].boxplot(plot_data)\n",
    "axs[1].set_title('Distribution of quot-tscores')\n",
    "\n",
    "# add x-tick labels\n",
    "plt.setp(axs, xticks=[y + 1 for y  in range(21)]\n",
    "        # ,xticklabels=['x1', 'x2', 'x3', 'x4']\n",
    "        )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Score by archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_delta_tscore_data = {}\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    \n",
    "    running_agg = []\n",
    "    if not config_archetypes[config] in archetype_delta_tscore_data.keys():\n",
    "        archetype_delta_tscore_data[config_archetypes[config]] = []\n",
    "    else:\n",
    "        running_agg = archetype_delta_tscore_data[config_archetypes[config]]\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = delta_tscore(gold,reference,altered)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            r = 0\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    archetype_delta_tscore_data[config_archetypes[config]] = running_agg\n",
    "    del running_agg\n",
    "\n",
    "#archetype_filtered_delta_tscore_data = [[a for a in x if a != 0] for x in delta_tscore_data]\n",
    "archetype_filtered_delta_tscore_data = [[a for a in x if abs(a)>0.025] for x in archetype_delta_tscore_data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = archetype_delta_tscore_data.values() \n",
    "# plot_data = archetype_filtered_delta_tscore_data \n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "# plot violin plot\n",
    "plt.violinplot(plot_data,\n",
    "                  showmeans=True,\n",
    "                  showmedians=True)\n",
    "plt.title('Distribution of delta-tscores \\n by archetype (raw)')\n",
    "\n",
    "plt.xticks(\n",
    "    ticks=[y + 1 for y  in range(len(set(config_archetypes.values())))],\n",
    "    labels=set(config_archetypes.values())\n",
    "          )\n",
    "plt.xlabel(\"Archetype of Transformations\")\n",
    "plt.ylabel(\"Delta-T-Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = archetype_filtered_delta_tscore_data \n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "# plot violin plot\n",
    "plt.violinplot(plot_data,\n",
    "                  showmeans=True,\n",
    "                  showmedians=True)\n",
    "plt.title('Distribution of delta-tscores \\n by archetype (filtered)')\n",
    "\n",
    "plt.xticks(\n",
    "    ticks=[y + 1 for y  in range(len(set(config_archetypes.values())))],\n",
    "    labels=set(config_archetypes.values())\n",
    "          )\n",
    "plt.xlabel(\"Archetype of Transformations\")\n",
    "plt.ylabel(\"Delta-T-Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### T-Robustness\n",
    "\n",
    "the t-robustness is the average of tscores on a configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n",
    "\n",
    "# plot_data = delta_tscore_data \n",
    "plot_data = filtered_delta_tscore_data \n",
    "\n",
    "# plot violin plot\n",
    "axs[0].violinplot(plot_data,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "axs[0].set_title('Distribution of delta-tscores')\n",
    "\n",
    "# plot box plot\n",
    "axs[1].boxplot(plot_data)\n",
    "axs[1].set_title('Distribution of delta-tscores')\n",
    "\n",
    "# add x-tick labels\n",
    "plt.setp(axs, xticks=[y + 1 for y  in range(21)]\n",
    "        # ,xticklabels=['x1', 'x2', 'x3', 'x4']\n",
    "        )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quot_tscore_data = []\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = tquot(gold,reference,altered)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            r = 0\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    quot_tscore_data.append(running_agg)\n",
    "    del running_agg\n",
    "    \n",
    "filtered_quot_tscore_data = [[a for a in x if a<2] for x in quot_tscore_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n",
    "\n",
    "# plot_data = quot_tscore_data \n",
    "plot_data = filtered_quot_tscore_data \n",
    "\n",
    "# plot violin plot\n",
    "axs[0].violinplot(plot_data,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "axs[0].set_title('Distribution of quot-tscores')\n",
    "\n",
    "# plot box plot\n",
    "axs[1].boxplot(plot_data)\n",
    "axs[1].set_title('Distribution of quot-tscores')\n",
    "\n",
    "# add x-tick labels\n",
    "plt.setp(axs, xticks=[y + 1 for y  in range(21)]\n",
    "        # ,xticklabels=['x1', 'x2', 'x3', 'x4']\n",
    "        )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n",
    "\n",
    "# plot_data = delta_tscore_data \n",
    "plot_data = filtered_delta_tscore_data \n",
    "\n",
    "# plot violin plot\n",
    "axs[0].violinplot(plot_data,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "axs[0].set_title('Distribution of delta-tscores')\n",
    "\n",
    "# plot box plot\n",
    "axs[1].boxplot(plot_data)\n",
    "axs[1].set_title('Distribution of delta-tscores')\n",
    "\n",
    "# add x-tick labels\n",
    "plt.setp(axs, xticks=[y + 1 for y  in range(21)]\n",
    "        # ,xticklabels=['x1', 'x2', 'x3', 'x4']\n",
    "        )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quot_tscore_data = []\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        r = ()\n",
    "        try:\n",
    "            r = tquot(gold,reference,altered)\n",
    "        except ZeroDivisionError: \n",
    "            #print(\"Error at \",index,config)\n",
    "            r = 0\n",
    "        running_agg.append(r)\n",
    "            \n",
    "    quot_tscore_data.append(running_agg)\n",
    "    del running_agg\n",
    "    \n",
    "filtered_quot_tscore_data = [[a for a in x if a<2] for x in quot_tscore_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n",
    "\n",
    "# plot_data = quot_tscore_data \n",
    "plot_data = filtered_quot_tscore_data \n",
    "\n",
    "# plot violin plot\n",
    "axs[0].violinplot(plot_data,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "axs[0].set_title('Distribution of quot-tscores')\n",
    "\n",
    "# plot box plot\n",
    "axs[1].boxplot(plot_data)\n",
    "axs[1].set_title('Distribution of quot-tscores')\n",
    "\n",
    "# add x-tick labels\n",
    "plt.setp(axs, xticks=[y + 1 for y  in range(21)]\n",
    "        # ,xticklabels=['x1', 'x2', 'x3', 'x4']\n",
    "        )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Robustness\n",
    "\n",
    "the t-robustness is the average of tscores on a configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        try:\n",
    "            running_agg.append(delta_tscore(gold,reference,altered))\n",
    "        except ZeroDivisionError: \n",
    "            print(\"Error at \",index,config)\n",
    "    \n",
    "    robustness = np.mean(running_agg)\n",
    "    variance = np.var(running_agg)\n",
    "    #print(config,\"delta-trobustness\",robustness)\n",
    "    #print(config,\"delta-t-variance\",variance)\n",
    "    results[config][\"delta-t-robustness\"]=robustness\n",
    "    results[config][\"delta-t-variance\"]=variance\n",
    "    del running_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in non_reference_configs:\n",
    "    running_agg = []\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        try:\n",
    "            running_agg.append(tquot(gold,reference,altered))\n",
    "        except ZeroDivisionError: \n",
    "            running_agg.append(0.0)\n",
    "    \n",
    "    robustness = np.mean(running_agg)\n",
    "    variance = np.var(running_agg)\n",
    "    #print(config,\"quot-trobustness\",robustness)\n",
    "    #print(config,\"quot-t-variance\",variance)\n",
    "    results[config][\"quot-t-robustness\"]=robustness\n",
    "    results[config][\"quot-t-variance\"]=variance\n",
    "    del running_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    robustness_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        robustness_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"delta-t-robustness\"]\n",
    "   \n",
    "robustness_data_df = pd.DataFrame.from_dict(robustness_data)\n",
    "\n",
    "plt.title(\"TRobustness given delta-tscore\")\n",
    "plt.ylabel(\"Delta-T-Robustness\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(robustness_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(robustness_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    robustness_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        robustness_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"quot-t-robustness\"]\n",
    "   \n",
    "robustness_data_df = pd.DataFrame.from_dict(robustness_data)\n",
    "\n",
    "plt.title(\"TRobustness given quot-tscore\")\n",
    "plt.ylabel(\"Quot-T-Robustness\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(robustness_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(robustness_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    variance_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        variance_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"delta-t-variance\"]\n",
    "   \n",
    "variance_data_df = pd.DataFrame.from_dict(variance_data)\n",
    "\n",
    "plt.title(\"TVariance given delta-tscore\")\n",
    "plt.ylabel(\"Delta-T-Variance\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(variance_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(variance_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variance_data = {}\n",
    "archetypes = set([results[k][\"archetype\"] for k in results.keys() if \"archetype\" in results[k].keys()])\n",
    "for archetype in archetypes:\n",
    "    variance_data[archetype]={}\n",
    "    relevant_configs = [k for k \n",
    "                        in results.keys() \n",
    "                        if \"archetype\" in results[k].keys() \n",
    "                        and results[k][\"archetype\"]==archetype]\n",
    "    for c in relevant_configs:\n",
    "        variance_data[archetype][results[c][\"properties\"][\"transformations\"]]=results[c][\"quot-t-variance\"]\n",
    "   \n",
    "variance_data_df = pd.DataFrame.from_dict(variance_data)\n",
    "\n",
    "plt.title(\"TVariance given quot-tscore\")\n",
    "plt.ylabel(\"quot-T-Variance\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(variance_data_df,marker=\"o\")\n",
    "\n",
    "plt.legend(variance_data_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in [key for key in results.keys() if \"reference\" != key]:\n",
    "    entries = len(results[config][\"results\"].keys())\n",
    "    diffs = 0\n",
    "    for index in list(results[config][\"results\"].keys()):\n",
    "        gold = results[\"reference\"][\"gold_results\"][index]\n",
    "        reference = results[\"reference\"][\"results\"][index]\n",
    "        altered = results[config][\"results\"][index]\n",
    "        \n",
    "        if reference != altered:\n",
    "            diffs = diffs + 1 \n",
    "            \n",
    "    results[config][\"diffs\"]=diffs\n",
    "    print(f\"{config} had {diffs} differences {round(float(diffs)/entries*100,2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
