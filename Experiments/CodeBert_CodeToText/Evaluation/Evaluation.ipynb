{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CodeBert Grid Experiment Evaluation\n",
    "\n",
    "Nice to see you around! Have a seat.\n",
    "Would you like a drink? Maybe a cigar?\n",
    "\n",
    "Make sure to have all required dependencies installed - they are listed in the [environment.yml](./environment.yml). \n",
    "You create a conda environment from the yml using \n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate Lampion-Codebert-Evaluation\n",
    "```\n",
    "\n",
    "Make sure to run your Jupyter Notebook from that environment! \n",
    "Otherwise you are (still) missing the dependencies. \n",
    "\n",
    "**OPTIONALLY** you can use the environment in which your jupter notebook is already running, with starting a new terminal (from jupyter) and run \n",
    "\n",
    "```\n",
    "conda env update --prefix ./env --file environment.yml  --prune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "# Homebrew Imports (python-file next to this)\n",
    "import bleu_evaluator as foreign_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data-Loading / Preparation\n",
    "\n",
    "Make sure that your dataset looks like described in the [Readme](./README.md), that is \n",
    "\n",
    "```\n",
    "./data\n",
    "    /GridExp_XY\n",
    "        /configs\n",
    "            /reference\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_0\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "            /config_1\n",
    "                config.properties\n",
    "                test_0.gold\n",
    "                test_0.output\n",
    "                bleu.txt (optional, can be created below)\n",
    "    ...\n",
    "```\n",
    "\n",
    "where the configs **must** be numbered to be correctly detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This runs the bleu-score upon the config files, creating the bleu.txt's \n",
    "# If your data package was provided including the txt you dont need to do this. \n",
    "# Existing bleu.txt's will be overwritten. \n",
    "\n",
    "#!./metric_runner.sh ./data/PreliminaryResults/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_directory = \"./data/PreliminaryResults\"\n",
    "\n",
    "print(f\"looking for results in {data_directory}\" )\n",
    "\n",
    "results={}\n",
    "\n",
    "for root,dirs,files in os.walk(data_directory):\n",
    "    for name in files:\n",
    "        if \".gold\" in name:\n",
    "            directory = os.path.basename(root)\n",
    "            results[directory]={}\n",
    "            \n",
    "            results[directory][\"result_file\"]=os.path.join(root,\"test_0.output\")\n",
    "            results[directory][\"gold_file\"]=os.path.join(root,\"test_0.gold\")\n",
    "            results[directory][\"bleu_file\"]=os.path.join(root,\"bleu.txt\")\n",
    "            if os.path.exists(os.path.join(root,\"config.properties\")):\n",
    "                results[directory][\"property_file\"]=os.path.join(root,\"config.properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_properties(filepath, sep='=', comment_char='#'):\n",
    "    \"\"\"\n",
    "    Read the file passed as parameter as a properties file.\n",
    "    \"\"\"\n",
    "    props = {}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            if l and not l.startswith(comment_char):\n",
    "                key_value = l.split(sep)\n",
    "                key = key_value[0].strip()\n",
    "                value = sep.join(key_value[1:]).strip().strip('\"') \n",
    "                props[key] = value \n",
    "    return props\n",
    "\n",
    "print(\"reading in property-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    if \"property_file\" in results[key].keys():\n",
    "        results[f\"{key}\"][\"properties\"]=load_properties(results[key][\"property_file\"])\n",
    "\n",
    "print(\"done reading the properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in result-files\")\n",
    "\n",
    "for key in results.keys():\n",
    "    result_file = results[key][\"result_file\"]\n",
    "    f = open(result_file)\n",
    "    lines=f.readlines()\n",
    "    results[key][\"results\"]={}\n",
    "    for l in lines:\n",
    "        num = int(l.split(\"\\t\")[0])\n",
    "        content = l.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"results\"][num] = content\n",
    "    f.close()\n",
    "    \n",
    "    gold_file = results[key]['gold_file']\n",
    "    gf = open(gold_file)\n",
    "    glines=gf.readlines()\n",
    "    results[key][\"gold_results\"]={}\n",
    "    for gl in glines:\n",
    "        num = int(gl.split(\"\\t\")[0])\n",
    "        content = gl.split(\"\\t\")[1]\n",
    "        content = content.strip()\n",
    "        results[key][\"gold_results\"][num] = content\n",
    "    gf.close()\n",
    "\n",
    "print(\"done reading the result files\")\n",
    "# Comment this in for inspection of results\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading in the bleu-scores\")\n",
    "\n",
    "for key in results.keys():\n",
    "    bleu_file = results[key][\"bleu_file\"]\n",
    "    f = open(bleu_file)\n",
    "    score=f.readlines()[0]\n",
    "    results[key][\"bleu\"]=float(score)\n",
    "    f.close()\n",
    "    \n",
    "print(\"done reading the bleu-scores\")\n",
    "\n",
    "#results[\"config_0\"][\"bleu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bleu-Scores\n",
    "\n",
    "In the following, the BLEU-scores will be calculated using the foreign libary. \n",
    "While there have been minor changes to standard-BLEU, it is the same as used in the original experiment.\n",
    "\n",
    "The aggregated BLEU-Scores will be stored to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Deprecated\n",
    "\"\"\"\n",
    "print(\"BLEU-Score of the un-altered Set:\")\n",
    "ref_bleu = foreign_bleu.bleuFromMaps(results[\"gold\"][\"results\"],results[\"reference\"][\"results\"])\n",
    "print(ref_bleu[0])\n",
    "print(\"BLEU-Score of the if-true config:\")\n",
    "c0_bleu = foreign_bleu.bleuFromMaps(results[\"gold\"][\"results\"],results[\"config_0\"][\"results\"])\n",
    "print(c0_bleu[0])\n",
    "print(\"BLEU-Score of the gold set:\")\n",
    "gold_bleu = foreign_bleu.bleuFromMaps(results[\"gold\"][\"results\"],results[\"gold\"][\"results\"])\n",
    "print(gold_bleu[0],\"(Should be 100)\")\n",
    "\n",
    "print(\"calculating bleu scores for all configs\")\n",
    "for key in results.keys():\n",
    "    if \"config\" in key or key == \"reference\":\n",
    "        bleu = foreign_bleu.bleuFromMaps(results[key][\"gold_results\"],results[key][\"results\"])\n",
    "        results[key][\"bleu\"]=bleu\n",
    "        \n",
    "results[\"config_5\"][\"bleu\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = {0:\"if-true\", 1:\"add-var(pseudo)\", 2:\"add-neutral\",3:\"add-var(random)\"}\n",
    "bleu_plots = {}\n",
    "\n",
    "num_property_files = len([1 for x in results.keys() if \"property_file\" in results[x].keys()])\n",
    "\n",
    "i = 0\n",
    "while i < num_property_files:\n",
    "    config_type = config_names[i // 3]\n",
    "    if config_type in bleu_plots.keys():\n",
    "        c = results[f\"config_{i}\"]\n",
    "        bleu_plots[config_type][int(c[\"properties\"][\"transformations\"])] = c[\"bleu\"]\n",
    "    else:\n",
    "        bleu_plots[config_type]={}\n",
    "        bleu_plots[config_type][0]=results[\"reference\"][\"bleu\"]\n",
    "        c = results[f\"config_{i}\"]\n",
    "        bleu_plots[config_type][int(c[\"properties\"][\"transformations\"])] = c[\"bleu\"]\n",
    "    i = i+1\n",
    "    \n",
    "bleu_df = pd.DataFrame.from_dict(bleu_plots)\n",
    "bleu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"BLEU-Score\")\n",
    "plt.xlabel(\"# Transformations\")\n",
    "\n",
    "plot = plt.plot(bleu_df,marker=\"o\")\n",
    "\n",
    "plt.legend(bleu_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = list(results[\"reference\"][\"gold_results\"].values())[0:10]\n",
    "v = list(results[\"reference\"][\"results\"].values())[0:10]\n",
    "w = list(results[\"config_0\"][\"results\"].values())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "while i < 10:\n",
    "    print(i,u[i],v[i],w[i])\n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[\"config_2\"][\"gold_results\"][1] )\n",
    "print(results[\"config_2\"][\"results\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_wrapper(sentenceA,sentenceB,ngram=1):\n",
    "    tokensA = nltk.word_tokenize(sentenceA)\n",
    "    tokensB = nltk.word_tokenize(sentenceB)\n",
    "\n",
    "    ngA_tokens = set(nltk.ngrams(tokensA, n=ngram))\n",
    "    ngB_tokens = set(nltk.ngrams(tokensB, n=ngram))\n",
    "    \n",
    "    return nltk.jaccard_distance(ngA_tokens, ngB_tokens)\n",
    "\n",
    "def closest_jaccard_match(sentence,references,ngram=1):\n",
    "    bestMatch = \"\"\n",
    "    bestJacc = 2\n",
    "    for r in references:\n",
    "        jacc = jaccard_wrapper(sentence,r)\n",
    "        if jacc < bestJacc:\n",
    "            bestJacc = jacc\n",
    "            bestMatch = r\n",
    "    return (bestMatch,bestJacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "goldies = list(results[\"reference\"][\"gold_results\"].values())\n",
    "\n",
    "sample_index = 120\n",
    "\n",
    "sample = results[\"config_4\"][\"results\"][sample_index]\n",
    "\n",
    "print(sample)\n",
    "print(closest_jaccard_match(sample,goldies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}